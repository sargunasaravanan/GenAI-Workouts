{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sargunasaravanan/GenAI-Workouts/blob/main/Demo_02_Generating_Fake_Images_with_Generative_Adversarial_Networks_(GANs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Demo: Generating Fake Images with Generative Adversarial Networks (GANs)__"
      ],
      "metadata": {
        "id": "hFjrWwbwmm--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Steps to Perform__\n",
        "\n",
        "Step 1: Import the Necessary Libraries\n",
        "\n",
        "Step 2: Load and Preprocess the Data\n",
        "\n",
        "Step 3: Build the Generator and Discriminator\n",
        "\n",
        "Step 4: Compile the Models\n",
        "\n",
        "Step 5: Train the Models\n",
        "\n",
        "Step 6: Execute the Training\n",
        "\n",
        "Step 7: Generate New Images and Evaluate the Model's Performance\n",
        "\n"
      ],
      "metadata": {
        "id": "4zAosTOTB-qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 1: Import the Necessary Libraries__"
      ],
      "metadata": {
        "id": "XWSHe4DSf-eN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MJUz4u4rdcOQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 2: Load and Preprocess the Data__\n",
        "\n",
        "- Load the MNIST dataset and preprocess it.\n",
        "- Preprocessing involves normalizing the data that can improve models' performance."
      ],
      "metadata": {
        "id": "-NlLSckbgHt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST data\n",
        "(X_train, _), (_, _) = mnist.load_data()\n",
        "# Normalize to between -1 and 1\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "X_train = np.expand_dims(X_train, axis=3)"
      ],
      "metadata": {
        "id": "gTjuO07Mdoe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a5f240-1a4e-4a6c-c448-2365275c558a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 3: Build the Generator and Discriminator__\n",
        "\n",
        "- Define the generator and discriminator models.\n",
        "- Generator takes a random noise vector as input and outputs an image.\n",
        "- Discriminator takes an image as input and outputs the probability of the image being real."
      ],
      "metadata": {
        "id": "nDoEhcHcgRmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "def create_generator():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=100, activation='relu'))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dense(784, activation='tanh'))\n",
        "    model.add(Reshape((28, 28, 1)))\n",
        "    return model\n",
        "\n",
        "# Discriminator\n",
        "def create_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "l0d5rZYeeUmP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 4: Compile the Models__\n",
        "\n",
        "- Compile the models, which involves defining the loss function and the optimizer.\n",
        "- The loss function evaluates the model's performance, while the optimizer aims to minimize the loss."
      ],
      "metadata": {
        "id": "lPpH5pVQgYic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n",
        "# Create and compile the discriminator\n",
        "discriminator = create_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Create and compile the generator\n",
        "generator = create_generator()\n",
        "generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# Create and compile the combined model\n",
        "discriminator.trainable = False\n",
        "gan_input = Input(shape=(100,))\n",
        "x = generator(gan_input)\n",
        "gan_output = discriminator(x)\n",
        "gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "KY1zu283eg0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c857d7-f3d7-4bf3-935f-192afd0d7a58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 5: Train the Models__\n",
        "\n",
        "- Train the model, which involves feeding data into the models and adjusting the weights of the models based on the output.\n",
        "- The primary aim is for the generator to create images indistinguishable from real images by the discriminator."
      ],
      "metadata": {
        "id": "HY3QwDoXgcPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=1, batch_size=128):\n",
        "    # Load the data\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    # Labels for the batch size and the test size\n",
        "    y_train_ones = np.ones((batch_size, 1))\n",
        "    y_train_zeros = np.zeros((batch_size, 1))\n",
        "    y_test_ones = np.ones((100, 1))\n",
        "\n",
        "    # Start training\n",
        "    for e in range(epochs):\n",
        "        for i in range(X_train.shape[0] // batch_size):\n",
        "            # Train Discriminator weights\n",
        "            discriminator.trainable = True\n",
        "\n",
        "            # Real samples\n",
        "            X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
        "            d_loss_real = discriminator.train_on_batch(x=X_batch, y=y_train_ones * (1 - 0.1 * np.random.rand(batch_size, 1)))\n",
        "\n",
        "            # Fake Samples\n",
        "            z_noise = np.random.normal(loc=0, scale=1, size=(batch_size, 100))\n",
        "            X_fake = generator.predict_on_batch(z_noise)\n",
        "            d_loss_fake = discriminator.train_on_batch(x=X_fake, y=y_train_zeros)\n",
        "\n",
        "            # Discriminator loss\n",
        "            d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "            # Train Generator weights\n",
        "            discriminator.trainable = False\n",
        "            g_loss = gan.train_on_batch(x=z_noise, y=y_train_ones)\n",
        "\n",
        "            print(f'Epoch: {e+1}, Batch: {i}, D Loss: {d_loss}, G Loss: {g_loss}')"
      ],
      "metadata": {
        "id": "IE9tbYPdenYp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 6: Execute the Training__"
      ],
      "metadata": {
        "id": "yFwC_WhRgo8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the train function\n",
        "train(epochs=50, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "74Qf18jCes5z",
        "outputId": "b6306cb9-b3e2-4b01-a471-dc7044410456"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 0, D Loss: 1.3961308002471924, G Loss: 0.6502684354782104\n",
            "Epoch: 1, Batch: 1, D Loss: 1.0792168378829956, G Loss: 0.5211355686187744\n",
            "Epoch: 1, Batch: 2, D Loss: 1.0375053882598877, G Loss: 0.6421970725059509\n",
            "Epoch: 1, Batch: 3, D Loss: 1.0580065250396729, G Loss: 0.6399136781692505\n",
            "Epoch: 1, Batch: 4, D Loss: 0.963042140007019, G Loss: 0.6671148538589478\n",
            "Epoch: 1, Batch: 5, D Loss: 0.8904962539672852, G Loss: 0.7935000061988831\n",
            "Epoch: 1, Batch: 6, D Loss: 0.8181964159011841, G Loss: 0.9663090109825134\n",
            "Epoch: 1, Batch: 7, D Loss: 0.7655268907546997, G Loss: 1.061079502105713\n",
            "Epoch: 1, Batch: 8, D Loss: 0.712197482585907, G Loss: 1.1410441398620605\n",
            "Epoch: 1, Batch: 9, D Loss: 0.6704995632171631, G Loss: 1.2674320936203003\n",
            "Epoch: 1, Batch: 10, D Loss: 0.6295087933540344, G Loss: 1.424060344696045\n",
            "Epoch: 1, Batch: 11, D Loss: 0.5990062952041626, G Loss: 1.5262314081192017\n",
            "Epoch: 1, Batch: 12, D Loss: 0.5680544376373291, G Loss: 1.6044033765792847\n",
            "Epoch: 1, Batch: 13, D Loss: 0.5427014827728271, G Loss: 1.6902248859405518\n",
            "Epoch: 1, Batch: 14, D Loss: 0.5187139511108398, G Loss: 1.7795056104660034\n",
            "Epoch: 1, Batch: 15, D Loss: 0.4979156255722046, G Loss: 1.8434773683547974\n",
            "Epoch: 1, Batch: 16, D Loss: 0.4799218773841858, G Loss: 1.8800350427627563\n",
            "Epoch: 1, Batch: 17, D Loss: 0.4651820957660675, G Loss: 1.8985402584075928\n",
            "Epoch: 1, Batch: 18, D Loss: 0.4529905915260315, G Loss: 1.9105497598648071\n",
            "Epoch: 1, Batch: 19, D Loss: 0.4411182701587677, G Loss: 1.9127531051635742\n",
            "Epoch: 1, Batch: 20, D Loss: 0.43219664692878723, G Loss: 1.901560664176941\n",
            "Epoch: 1, Batch: 21, D Loss: 0.423898845911026, G Loss: 1.8850066661834717\n",
            "Epoch: 1, Batch: 22, D Loss: 0.4176446497440338, G Loss: 1.8658019304275513\n",
            "Epoch: 1, Batch: 23, D Loss: 0.4125123620033264, G Loss: 1.8431596755981445\n",
            "Epoch: 1, Batch: 24, D Loss: 0.4108205735683441, G Loss: 1.8148454427719116\n",
            "Epoch: 1, Batch: 25, D Loss: 0.4106905460357666, G Loss: 1.7845818996429443\n",
            "Epoch: 1, Batch: 26, D Loss: 0.4126415252685547, G Loss: 1.752776861190796\n",
            "Epoch: 1, Batch: 27, D Loss: 0.4152286648750305, G Loss: 1.7228281497955322\n",
            "Epoch: 1, Batch: 28, D Loss: 0.4155164361000061, G Loss: 1.6967030763626099\n",
            "Epoch: 1, Batch: 29, D Loss: 0.4166308045387268, G Loss: 1.6723077297210693\n",
            "Epoch: 1, Batch: 30, D Loss: 0.4151816964149475, G Loss: 1.65382981300354\n",
            "Epoch: 1, Batch: 31, D Loss: 0.41263800859451294, G Loss: 1.64637291431427\n",
            "Epoch: 1, Batch: 32, D Loss: 0.4079260230064392, G Loss: 1.6523131132125854\n",
            "Epoch: 1, Batch: 33, D Loss: 0.4044226408004761, G Loss: 1.659785509109497\n",
            "Epoch: 1, Batch: 34, D Loss: 0.40041953325271606, G Loss: 1.671349287033081\n",
            "Epoch: 1, Batch: 35, D Loss: 0.39737626910209656, G Loss: 1.6868624687194824\n",
            "Epoch: 1, Batch: 36, D Loss: 0.3936173915863037, G Loss: 1.7049803733825684\n",
            "Epoch: 1, Batch: 37, D Loss: 0.3882812261581421, G Loss: 1.7239928245544434\n",
            "Epoch: 1, Batch: 38, D Loss: 0.4071587026119232, G Loss: 1.7236734628677368\n",
            "Epoch: 1, Batch: 39, D Loss: 0.4066835045814514, G Loss: 1.7137930393218994\n",
            "Epoch: 1, Batch: 40, D Loss: 0.4111560881137848, G Loss: 1.699069857597351\n",
            "Epoch: 1, Batch: 41, D Loss: 0.41747409105300903, G Loss: 1.6819472312927246\n",
            "Epoch: 1, Batch: 42, D Loss: 0.42415210604667664, G Loss: 1.6650587320327759\n",
            "Epoch: 1, Batch: 43, D Loss: 0.42982396483421326, G Loss: 1.650434136390686\n",
            "Epoch: 1, Batch: 44, D Loss: 0.4329898953437805, G Loss: 1.6393400430679321\n",
            "Epoch: 1, Batch: 45, D Loss: 0.43259602785110474, G Loss: 1.6322492361068726\n",
            "Epoch: 1, Batch: 46, D Loss: 0.4291027784347534, G Loss: 1.6291942596435547\n",
            "Epoch: 1, Batch: 47, D Loss: 0.4318263530731201, G Loss: 1.6256237030029297\n",
            "Epoch: 1, Batch: 48, D Loss: 0.4295586943626404, G Loss: 1.6222286224365234\n",
            "Epoch: 1, Batch: 49, D Loss: 0.43028807640075684, G Loss: 1.619606375694275\n",
            "Epoch: 1, Batch: 50, D Loss: 0.43164822459220886, G Loss: 1.6183056831359863\n",
            "Epoch: 1, Batch: 51, D Loss: 0.4337098002433777, G Loss: 1.6190779209136963\n",
            "Epoch: 1, Batch: 52, D Loss: 0.4354580044746399, G Loss: 1.6227394342422485\n",
            "Epoch: 1, Batch: 53, D Loss: 0.43573808670043945, G Loss: 1.6309503316879272\n",
            "Epoch: 1, Batch: 54, D Loss: 0.4336557388305664, G Loss: 1.6459585428237915\n",
            "Epoch: 1, Batch: 55, D Loss: 0.43338602781295776, G Loss: 1.6524484157562256\n",
            "Epoch: 1, Batch: 56, D Loss: 0.4318958818912506, G Loss: 1.6553046703338623\n",
            "Epoch: 1, Batch: 57, D Loss: 0.43162113428115845, G Loss: 1.6579803228378296\n",
            "Epoch: 1, Batch: 58, D Loss: 0.4315735101699829, G Loss: 1.6624406576156616\n",
            "Epoch: 1, Batch: 59, D Loss: 0.4316927492618561, G Loss: 1.6712398529052734\n",
            "Epoch: 1, Batch: 60, D Loss: 0.4306822419166565, G Loss: 1.6871075630187988\n",
            "Epoch: 1, Batch: 61, D Loss: 0.42824292182922363, G Loss: 1.712550163269043\n",
            "Epoch: 1, Batch: 62, D Loss: 0.42443084716796875, G Loss: 1.7491466999053955\n",
            "Epoch: 1, Batch: 63, D Loss: 0.41952580213546753, G Loss: 1.7960346937179565\n",
            "Epoch: 1, Batch: 64, D Loss: 0.4305822551250458, G Loss: 1.8094274997711182\n",
            "Epoch: 1, Batch: 65, D Loss: 0.42792612314224243, G Loss: 1.7971433401107788\n",
            "Epoch: 1, Batch: 66, D Loss: 0.42985254526138306, G Loss: 1.7789899110794067\n",
            "Epoch: 1, Batch: 67, D Loss: 0.43481600284576416, G Loss: 1.7604687213897705\n",
            "Epoch: 1, Batch: 68, D Loss: 0.44074589014053345, G Loss: 1.7422367334365845\n",
            "Epoch: 1, Batch: 69, D Loss: 0.4467166066169739, G Loss: 1.7246533632278442\n",
            "Epoch: 1, Batch: 70, D Loss: 0.4520810842514038, G Loss: 1.708045482635498\n",
            "Epoch: 1, Batch: 71, D Loss: 0.45668965578079224, G Loss: 1.692696452140808\n",
            "Epoch: 1, Batch: 72, D Loss: 0.4592611789703369, G Loss: 1.678844928741455\n",
            "Epoch: 1, Batch: 73, D Loss: 0.4601133167743683, G Loss: 1.6666772365570068\n",
            "Epoch: 1, Batch: 74, D Loss: 0.4595271348953247, G Loss: 1.6563528776168823\n",
            "Epoch: 1, Batch: 75, D Loss: 0.4595569372177124, G Loss: 1.6467201709747314\n",
            "Epoch: 1, Batch: 76, D Loss: 0.4595910310745239, G Loss: 1.636907696723938\n",
            "Epoch: 1, Batch: 77, D Loss: 0.4583774209022522, G Loss: 1.6272242069244385\n",
            "Epoch: 1, Batch: 78, D Loss: 0.4572945535182953, G Loss: 1.6182609796524048\n",
            "Epoch: 1, Batch: 79, D Loss: 0.45617568492889404, G Loss: 1.6110782623291016\n",
            "Epoch: 1, Batch: 80, D Loss: 0.45470085740089417, G Loss: 1.6085916757583618\n",
            "Epoch: 1, Batch: 81, D Loss: 0.45219457149505615, G Loss: 1.6159684658050537\n",
            "Epoch: 1, Batch: 82, D Loss: 0.4493297338485718, G Loss: 1.6320562362670898\n",
            "Epoch: 1, Batch: 83, D Loss: 0.44602376222610474, G Loss: 1.651641607284546\n",
            "Epoch: 1, Batch: 84, D Loss: 0.4422324597835541, G Loss: 1.6756889820098877\n",
            "Epoch: 1, Batch: 85, D Loss: 0.4384978413581848, G Loss: 1.7052863836288452\n",
            "Epoch: 1, Batch: 86, D Loss: 0.43494081497192383, G Loss: 1.7413567304611206\n",
            "Epoch: 1, Batch: 87, D Loss: 0.43126386404037476, G Loss: 1.782928466796875\n",
            "Epoch: 1, Batch: 88, D Loss: 0.42772015929222107, G Loss: 1.8206324577331543\n",
            "Epoch: 1, Batch: 89, D Loss: 0.42424845695495605, G Loss: 1.8535140752792358\n",
            "Epoch: 1, Batch: 90, D Loss: 0.4208179712295532, G Loss: 1.8821773529052734\n",
            "Epoch: 1, Batch: 91, D Loss: 0.41743919253349304, G Loss: 1.906572699546814\n",
            "Epoch: 1, Batch: 92, D Loss: 0.4143253564834595, G Loss: 1.9221268892288208\n",
            "Epoch: 1, Batch: 93, D Loss: 0.41133350133895874, G Loss: 1.9355121850967407\n",
            "Epoch: 1, Batch: 94, D Loss: 0.40873467922210693, G Loss: 1.9420585632324219\n",
            "Epoch: 1, Batch: 95, D Loss: 0.4064715504646301, G Loss: 1.9494003057479858\n",
            "Epoch: 1, Batch: 96, D Loss: 0.4050973653793335, G Loss: 1.9437412023544312\n",
            "Epoch: 1, Batch: 97, D Loss: 0.4042106866836548, G Loss: 1.9452078342437744\n",
            "Epoch: 1, Batch: 98, D Loss: 0.40255922079086304, G Loss: 1.966431736946106\n",
            "Epoch: 1, Batch: 99, D Loss: 0.419399619102478, G Loss: 1.9505085945129395\n",
            "Epoch: 1, Batch: 100, D Loss: 0.42375221848487854, G Loss: 1.9332914352416992\n",
            "Epoch: 1, Batch: 101, D Loss: 0.4316890835762024, G Loss: 1.915895700454712\n",
            "Epoch: 1, Batch: 102, D Loss: 0.4412193298339844, G Loss: 1.8989369869232178\n",
            "Epoch: 1, Batch: 103, D Loss: 0.4504297971725464, G Loss: 1.8830506801605225\n",
            "Epoch: 1, Batch: 104, D Loss: 0.4568639397621155, G Loss: 1.869382381439209\n",
            "Epoch: 1, Batch: 105, D Loss: 0.45924851298332214, G Loss: 1.8599255084991455\n",
            "Epoch: 1, Batch: 106, D Loss: 0.46162688732147217, G Loss: 1.8494958877563477\n",
            "Epoch: 1, Batch: 107, D Loss: 0.46137529611587524, G Loss: 1.8383924961090088\n",
            "Epoch: 1, Batch: 108, D Loss: 0.46150946617126465, G Loss: 1.8275747299194336\n",
            "Epoch: 1, Batch: 109, D Loss: 0.46155720949172974, G Loss: 1.8178199529647827\n",
            "Epoch: 1, Batch: 110, D Loss: 0.4612615704536438, G Loss: 1.8091838359832764\n",
            "Epoch: 1, Batch: 111, D Loss: 0.46147865056991577, G Loss: 1.7998332977294922\n",
            "Epoch: 1, Batch: 112, D Loss: 0.4610636830329895, G Loss: 1.7903982400894165\n",
            "Epoch: 1, Batch: 113, D Loss: 0.46076416969299316, G Loss: 1.7816529273986816\n",
            "Epoch: 1, Batch: 114, D Loss: 0.46026408672332764, G Loss: 1.773990273475647\n",
            "Epoch: 1, Batch: 115, D Loss: 0.4598396420478821, G Loss: 1.7665526866912842\n",
            "Epoch: 1, Batch: 116, D Loss: 0.45927026867866516, G Loss: 1.758970022201538\n",
            "Epoch: 1, Batch: 117, D Loss: 0.4585322439670563, G Loss: 1.7516793012619019\n",
            "Epoch: 1, Batch: 118, D Loss: 0.4577403664588928, G Loss: 1.7453213930130005\n",
            "Epoch: 1, Batch: 119, D Loss: 0.4568697214126587, G Loss: 1.7397034168243408\n",
            "Epoch: 1, Batch: 120, D Loss: 0.4560336470603943, G Loss: 1.7339468002319336\n",
            "Epoch: 1, Batch: 121, D Loss: 0.45505058765411377, G Loss: 1.7287172079086304\n",
            "Epoch: 1, Batch: 122, D Loss: 0.45400965213775635, G Loss: 1.7244502305984497\n",
            "Epoch: 1, Batch: 123, D Loss: 0.4529987573623657, G Loss: 1.720150113105774\n",
            "Epoch: 1, Batch: 124, D Loss: 0.4522166848182678, G Loss: 1.715323805809021\n",
            "Epoch: 1, Batch: 125, D Loss: 0.45163583755493164, G Loss: 1.710532307624817\n",
            "Epoch: 1, Batch: 126, D Loss: 0.4517565369606018, G Loss: 1.7047386169433594\n",
            "Epoch: 1, Batch: 127, D Loss: 0.45271021127700806, G Loss: 1.6973334550857544\n",
            "Epoch: 1, Batch: 128, D Loss: 0.45439285039901733, G Loss: 1.6904867887496948\n",
            "Epoch: 1, Batch: 129, D Loss: 0.45816799998283386, G Loss: 1.6816142797470093\n",
            "Epoch: 1, Batch: 130, D Loss: 0.4601513743400574, G Loss: 1.6773260831832886\n",
            "Epoch: 1, Batch: 131, D Loss: 0.4631316065788269, G Loss: 1.6714380979537964\n",
            "Epoch: 1, Batch: 132, D Loss: 0.4633867144584656, G Loss: 1.6699697971343994\n",
            "Epoch: 1, Batch: 133, D Loss: 0.4631752073764801, G Loss: 1.673477053642273\n",
            "Epoch: 1, Batch: 134, D Loss: 0.462735116481781, G Loss: 1.67445707321167\n",
            "Epoch: 1, Batch: 135, D Loss: 0.46166732907295227, G Loss: 1.6786413192749023\n",
            "Epoch: 1, Batch: 136, D Loss: 0.4605593681335449, G Loss: 1.6875361204147339\n",
            "Epoch: 1, Batch: 137, D Loss: 0.46046698093414307, G Loss: 1.68904447555542\n",
            "Epoch: 1, Batch: 138, D Loss: 0.46005958318710327, G Loss: 1.6950857639312744\n",
            "Epoch: 1, Batch: 139, D Loss: 0.46115243434906006, G Loss: 1.698249101638794\n",
            "Epoch: 1, Batch: 140, D Loss: 0.4626171588897705, G Loss: 1.6978378295898438\n",
            "Epoch: 1, Batch: 141, D Loss: 0.46295884251594543, G Loss: 1.7043801546096802\n",
            "Epoch: 1, Batch: 142, D Loss: 0.46310997009277344, G Loss: 1.7110315561294556\n",
            "Epoch: 1, Batch: 143, D Loss: 0.4613379240036011, G Loss: 1.7197613716125488\n",
            "Epoch: 1, Batch: 144, D Loss: 0.45934662222862244, G Loss: 1.7338323593139648\n",
            "Epoch: 1, Batch: 145, D Loss: 0.4572962522506714, G Loss: 1.7494750022888184\n",
            "Epoch: 1, Batch: 146, D Loss: 0.45516565442085266, G Loss: 1.7664719820022583\n",
            "Epoch: 1, Batch: 147, D Loss: 0.4529891610145569, G Loss: 1.7811508178710938\n",
            "Epoch: 1, Batch: 148, D Loss: 0.4509662091732025, G Loss: 1.7935254573822021\n",
            "Epoch: 1, Batch: 149, D Loss: 0.4492330849170685, G Loss: 1.800218939781189\n",
            "Epoch: 1, Batch: 150, D Loss: 0.4479125440120697, G Loss: 1.8022464513778687\n",
            "Epoch: 1, Batch: 151, D Loss: 0.4469093382358551, G Loss: 1.8032323122024536\n",
            "Epoch: 1, Batch: 152, D Loss: 0.4458404779434204, G Loss: 1.8032495975494385\n",
            "Epoch: 1, Batch: 153, D Loss: 0.4447926878929138, G Loss: 1.8041348457336426\n",
            "Epoch: 1, Batch: 154, D Loss: 0.4433993697166443, G Loss: 1.8057713508605957\n",
            "Epoch: 1, Batch: 155, D Loss: 0.4417853355407715, G Loss: 1.8092807531356812\n",
            "Epoch: 1, Batch: 156, D Loss: 0.44005221128463745, G Loss: 1.8149932622909546\n",
            "Epoch: 1, Batch: 157, D Loss: 0.4382205605506897, G Loss: 1.8223282098770142\n",
            "Epoch: 1, Batch: 158, D Loss: 0.43631571531295776, G Loss: 1.8307157754898071\n",
            "Epoch: 1, Batch: 159, D Loss: 0.43445098400115967, G Loss: 1.8400824069976807\n",
            "Epoch: 1, Batch: 160, D Loss: 0.4325404465198517, G Loss: 1.8503738641738892\n",
            "Epoch: 1, Batch: 161, D Loss: 0.4306599497795105, G Loss: 1.8610626459121704\n",
            "Epoch: 1, Batch: 162, D Loss: 0.4287431836128235, G Loss: 1.8720099925994873\n",
            "Epoch: 1, Batch: 163, D Loss: 0.4268971085548401, G Loss: 1.88318932056427\n",
            "Epoch: 1, Batch: 164, D Loss: 0.42503243684768677, G Loss: 1.8945181369781494\n",
            "Epoch: 1, Batch: 165, D Loss: 0.42324188351631165, G Loss: 1.9053804874420166\n",
            "Epoch: 1, Batch: 166, D Loss: 0.42144274711608887, G Loss: 1.9157333374023438\n",
            "Epoch: 1, Batch: 167, D Loss: 0.4196605682373047, G Loss: 1.9254857301712036\n",
            "Epoch: 1, Batch: 168, D Loss: 0.41790610551834106, G Loss: 1.934478759765625\n",
            "Epoch: 1, Batch: 169, D Loss: 0.4161738455295563, G Loss: 1.9427536725997925\n",
            "Epoch: 1, Batch: 170, D Loss: 0.4144752025604248, G Loss: 1.9493013620376587\n",
            "Epoch: 1, Batch: 171, D Loss: 0.4128485918045044, G Loss: 1.9546489715576172\n",
            "Epoch: 1, Batch: 172, D Loss: 0.411307156085968, G Loss: 1.9584583044052124\n",
            "Epoch: 1, Batch: 173, D Loss: 0.4099017381668091, G Loss: 1.9607301950454712\n",
            "Epoch: 1, Batch: 174, D Loss: 0.4086529016494751, G Loss: 1.9608771800994873\n",
            "Epoch: 1, Batch: 175, D Loss: 0.40770241618156433, G Loss: 1.9587979316711426\n",
            "Epoch: 1, Batch: 176, D Loss: 0.4069737195968628, G Loss: 1.955440878868103\n",
            "Epoch: 1, Batch: 177, D Loss: 0.4064016342163086, G Loss: 1.9519007205963135\n",
            "Epoch: 1, Batch: 178, D Loss: 0.40593820810317993, G Loss: 1.9478962421417236\n",
            "Epoch: 1, Batch: 179, D Loss: 0.40606141090393066, G Loss: 1.941859483718872\n",
            "Epoch: 1, Batch: 180, D Loss: 0.406496524810791, G Loss: 1.9361640214920044\n",
            "Epoch: 1, Batch: 181, D Loss: 0.40813398361206055, G Loss: 1.928562045097351\n",
            "Epoch: 1, Batch: 182, D Loss: 0.4086132049560547, G Loss: 1.9268540143966675\n",
            "Epoch: 1, Batch: 183, D Loss: 0.4083310663700104, G Loss: 1.9299625158309937\n",
            "Epoch: 1, Batch: 184, D Loss: 0.41021406650543213, G Loss: 1.9223588705062866\n",
            "Epoch: 1, Batch: 185, D Loss: 0.41165751218795776, G Loss: 1.916157841682434\n",
            "Epoch: 1, Batch: 186, D Loss: 0.41225361824035645, G Loss: 1.9198633432388306\n",
            "Epoch: 1, Batch: 187, D Loss: 0.4156923294067383, G Loss: 1.9143301248550415\n",
            "Epoch: 1, Batch: 188, D Loss: 0.4158703684806824, G Loss: 1.909870982170105\n",
            "Epoch: 1, Batch: 189, D Loss: 0.416337251663208, G Loss: 1.9147906303405762\n",
            "Epoch: 1, Batch: 190, D Loss: 0.4153568744659424, G Loss: 1.931404948234558\n",
            "Epoch: 1, Batch: 191, D Loss: 0.41433078050613403, G Loss: 1.9432034492492676\n",
            "Epoch: 1, Batch: 192, D Loss: 0.4137471318244934, G Loss: 1.9406577348709106\n",
            "Epoch: 1, Batch: 193, D Loss: 0.41439974308013916, G Loss: 1.9401850700378418\n",
            "Epoch: 1, Batch: 194, D Loss: 0.4167819619178772, G Loss: 1.941357135772705\n",
            "Epoch: 1, Batch: 195, D Loss: 0.4184447228908539, G Loss: 1.940659999847412\n",
            "Epoch: 1, Batch: 196, D Loss: 0.4174070656299591, G Loss: 1.947595238685608\n",
            "Epoch: 1, Batch: 197, D Loss: 0.4161728620529175, G Loss: 1.9623998403549194\n",
            "Epoch: 1, Batch: 198, D Loss: 0.4147401452064514, G Loss: 1.97983980178833\n",
            "Epoch: 1, Batch: 199, D Loss: 0.41377753019332886, G Loss: 1.985345721244812\n",
            "Epoch: 1, Batch: 200, D Loss: 0.4130988121032715, G Loss: 1.9894115924835205\n",
            "Epoch: 1, Batch: 201, D Loss: 0.4130425453186035, G Loss: 1.9948396682739258\n",
            "Epoch: 1, Batch: 202, D Loss: 0.41460204124450684, G Loss: 1.9925711154937744\n",
            "Epoch: 1, Batch: 203, D Loss: 0.4147754907608032, G Loss: 1.99703848361969\n",
            "Epoch: 1, Batch: 204, D Loss: 0.41497868299484253, G Loss: 1.999758243560791\n",
            "Epoch: 1, Batch: 205, D Loss: 0.4139896035194397, G Loss: 2.0025734901428223\n",
            "Epoch: 1, Batch: 206, D Loss: 0.412899911403656, G Loss: 2.007589340209961\n",
            "Epoch: 1, Batch: 207, D Loss: 0.4116651117801666, G Loss: 2.014721632003784\n",
            "Epoch: 1, Batch: 208, D Loss: 0.41046565771102905, G Loss: 2.0226924419403076\n",
            "Epoch: 1, Batch: 209, D Loss: 0.40923959016799927, G Loss: 2.0290470123291016\n",
            "Epoch: 1, Batch: 210, D Loss: 0.40806451439857483, G Loss: 2.0328681468963623\n",
            "Epoch: 1, Batch: 211, D Loss: 0.40692538022994995, G Loss: 2.035248041152954\n",
            "Epoch: 1, Batch: 212, D Loss: 0.4058570861816406, G Loss: 2.036817789077759\n",
            "Epoch: 1, Batch: 213, D Loss: 0.40478622913360596, G Loss: 2.0378522872924805\n",
            "Epoch: 1, Batch: 214, D Loss: 0.4037337303161621, G Loss: 2.0388875007629395\n",
            "Epoch: 1, Batch: 215, D Loss: 0.4028194546699524, G Loss: 2.039341449737549\n",
            "Epoch: 1, Batch: 216, D Loss: 0.4018113613128662, G Loss: 2.0392775535583496\n",
            "Epoch: 1, Batch: 217, D Loss: 0.4008733928203583, G Loss: 2.0392470359802246\n",
            "Epoch: 1, Batch: 218, D Loss: 0.39997047185897827, G Loss: 2.0396335124969482\n",
            "Epoch: 1, Batch: 219, D Loss: 0.39906445145606995, G Loss: 2.040149211883545\n",
            "Epoch: 1, Batch: 220, D Loss: 0.3982478380203247, G Loss: 2.0396339893341064\n",
            "Epoch: 1, Batch: 221, D Loss: 0.3973842263221741, G Loss: 2.038597345352173\n",
            "Epoch: 1, Batch: 222, D Loss: 0.39661115407943726, G Loss: 2.0378522872924805\n",
            "Epoch: 1, Batch: 223, D Loss: 0.395885705947876, G Loss: 2.0371999740600586\n",
            "Epoch: 1, Batch: 224, D Loss: 0.3952546715736389, G Loss: 2.0358948707580566\n",
            "Epoch: 1, Batch: 225, D Loss: 0.394603431224823, G Loss: 2.034189462661743\n",
            "Epoch: 1, Batch: 226, D Loss: 0.3941119313240051, G Loss: 2.0321168899536133\n",
            "Epoch: 1, Batch: 227, D Loss: 0.39359188079833984, G Loss: 2.0297069549560547\n",
            "Epoch: 1, Batch: 228, D Loss: 0.3931664228439331, G Loss: 2.027681589126587\n",
            "Epoch: 1, Batch: 229, D Loss: 0.3926795721054077, G Loss: 2.025996208190918\n",
            "Epoch: 1, Batch: 230, D Loss: 0.3923407196998596, G Loss: 2.023627519607544\n",
            "Epoch: 1, Batch: 231, D Loss: 0.3919147849082947, G Loss: 2.0221614837646484\n",
            "Epoch: 1, Batch: 232, D Loss: 0.39144301414489746, G Loss: 2.0220723152160645\n",
            "Epoch: 1, Batch: 233, D Loss: 0.39177149534225464, G Loss: 2.0179531574249268\n",
            "Epoch: 1, Batch: 234, D Loss: 0.39248377084732056, G Loss: 2.0152745246887207\n",
            "Epoch: 1, Batch: 235, D Loss: 0.3928126096725464, G Loss: 2.0226247310638428\n",
            "Epoch: 1, Batch: 236, D Loss: 0.39412251114845276, G Loss: 2.02020525932312\n",
            "Epoch: 1, Batch: 237, D Loss: 0.39437156915664673, G Loss: 2.0195114612579346\n",
            "Epoch: 1, Batch: 238, D Loss: 0.3944432735443115, G Loss: 2.0294227600097656\n",
            "Epoch: 1, Batch: 239, D Loss: 0.39413583278656006, G Loss: 2.034148693084717\n",
            "Epoch: 1, Batch: 240, D Loss: 0.3946267366409302, G Loss: 2.032158374786377\n",
            "Epoch: 1, Batch: 241, D Loss: 0.39592429995536804, G Loss: 2.035926342010498\n",
            "Epoch: 1, Batch: 242, D Loss: 0.3989262878894806, G Loss: 2.037998914718628\n",
            "Epoch: 1, Batch: 243, D Loss: 0.3991636633872986, G Loss: 2.0441536903381348\n",
            "Epoch: 1, Batch: 244, D Loss: 0.3988223373889923, G Loss: 2.054314613342285\n",
            "Epoch: 1, Batch: 245, D Loss: 0.3978397250175476, G Loss: 2.0667648315429688\n",
            "Epoch: 1, Batch: 246, D Loss: 0.39684605598449707, G Loss: 2.0802712440490723\n",
            "Epoch: 1, Batch: 247, D Loss: 0.3959352672100067, G Loss: 2.0919175148010254\n",
            "Epoch: 1, Batch: 248, D Loss: 0.39544856548309326, G Loss: 2.0988991260528564\n",
            "Epoch: 1, Batch: 249, D Loss: 0.39559251070022583, G Loss: 2.1053237915039062\n",
            "Epoch: 1, Batch: 250, D Loss: 0.3967084288597107, G Loss: 2.109870433807373\n",
            "Epoch: 1, Batch: 251, D Loss: 0.3982676863670349, G Loss: 2.113036632537842\n",
            "Epoch: 1, Batch: 252, D Loss: 0.398555189371109, G Loss: 2.1205294132232666\n",
            "Epoch: 1, Batch: 253, D Loss: 0.3992116451263428, G Loss: 2.122258424758911\n",
            "Epoch: 1, Batch: 254, D Loss: 0.40014076232910156, G Loss: 2.123278856277466\n",
            "Epoch: 1, Batch: 255, D Loss: 0.40238672494888306, G Loss: 2.1232903003692627\n",
            "Epoch: 1, Batch: 256, D Loss: 0.4037413001060486, G Loss: 2.125481605529785\n",
            "Epoch: 1, Batch: 257, D Loss: 0.4048081636428833, G Loss: 2.1277942657470703\n",
            "Epoch: 1, Batch: 258, D Loss: 0.40467387437820435, G Loss: 2.13102126121521\n",
            "Epoch: 1, Batch: 259, D Loss: 0.4043836295604706, G Loss: 2.134413003921509\n",
            "Epoch: 1, Batch: 260, D Loss: 0.40374481678009033, G Loss: 2.138720750808716\n",
            "Epoch: 1, Batch: 261, D Loss: 0.40295884013175964, G Loss: 2.1445369720458984\n",
            "Epoch: 1, Batch: 262, D Loss: 0.4021698236465454, G Loss: 2.1508641242980957\n",
            "Epoch: 1, Batch: 263, D Loss: 0.4014478325843811, G Loss: 2.158536434173584\n",
            "Epoch: 1, Batch: 264, D Loss: 0.4009093642234802, G Loss: 2.1672158241271973\n",
            "Epoch: 1, Batch: 265, D Loss: 0.4009188115596771, G Loss: 2.175762891769409\n",
            "Epoch: 1, Batch: 266, D Loss: 0.4006555676460266, G Loss: 2.187769651412964\n",
            "Epoch: 1, Batch: 267, D Loss: 0.4010438621044159, G Loss: 2.198192834854126\n",
            "Epoch: 1, Batch: 268, D Loss: 0.4009738564491272, G Loss: 2.212432384490967\n",
            "Epoch: 1, Batch: 269, D Loss: 0.4011198878288269, G Loss: 2.223933458328247\n",
            "Epoch: 1, Batch: 270, D Loss: 0.40189969539642334, G Loss: 2.2391538619995117\n",
            "Epoch: 1, Batch: 271, D Loss: 0.40408188104629517, G Loss: 2.249699115753174\n",
            "Epoch: 1, Batch: 272, D Loss: 0.4068179726600647, G Loss: 2.2614381313323975\n",
            "Epoch: 1, Batch: 273, D Loss: 0.4093484580516815, G Loss: 2.2728700637817383\n",
            "Epoch: 1, Batch: 274, D Loss: 0.4094223976135254, G Loss: 2.2813053131103516\n",
            "Epoch: 1, Batch: 275, D Loss: 0.4087533950805664, G Loss: 2.294968843460083\n",
            "Epoch: 1, Batch: 276, D Loss: 0.4078148603439331, G Loss: 2.3156235218048096\n",
            "Epoch: 1, Batch: 277, D Loss: 0.40691423416137695, G Loss: 2.337583541870117\n",
            "Epoch: 1, Batch: 278, D Loss: 0.40600693225860596, G Loss: 2.3582584857940674\n",
            "Epoch: 1, Batch: 279, D Loss: 0.40510788559913635, G Loss: 2.3751585483551025\n",
            "Epoch: 1, Batch: 280, D Loss: 0.4042344093322754, G Loss: 2.386831283569336\n",
            "Epoch: 1, Batch: 281, D Loss: 0.4034156799316406, G Loss: 2.394200563430786\n",
            "Epoch: 1, Batch: 282, D Loss: 0.40283507108688354, G Loss: 2.402056932449341\n",
            "Epoch: 1, Batch: 283, D Loss: 0.4022846221923828, G Loss: 2.4129388332366943\n",
            "Epoch: 1, Batch: 284, D Loss: 0.4013959765434265, G Loss: 2.4254372119903564\n",
            "Epoch: 1, Batch: 285, D Loss: 0.4004628658294678, G Loss: 2.4373953342437744\n",
            "Epoch: 1, Batch: 286, D Loss: 0.3996601104736328, G Loss: 2.4465742111206055\n",
            "Epoch: 1, Batch: 287, D Loss: 0.39884287118911743, G Loss: 2.4588401317596436\n",
            "Epoch: 1, Batch: 288, D Loss: 0.39822518825531006, G Loss: 2.47044038772583\n",
            "Epoch: 1, Batch: 289, D Loss: 0.3978120684623718, G Loss: 2.4825870990753174\n",
            "Epoch: 1, Batch: 290, D Loss: 0.39782530069351196, G Loss: 2.4957470893859863\n",
            "Epoch: 1, Batch: 291, D Loss: 0.39824312925338745, G Loss: 2.507932186126709\n",
            "Epoch: 1, Batch: 292, D Loss: 0.40002086758613586, G Loss: 2.516120195388794\n",
            "Epoch: 1, Batch: 293, D Loss: 0.4018297493457794, G Loss: 2.524543285369873\n",
            "Epoch: 1, Batch: 294, D Loss: 0.40469223260879517, G Loss: 2.5288281440734863\n",
            "Epoch: 1, Batch: 295, D Loss: 0.40636515617370605, G Loss: 2.5325214862823486\n",
            "Epoch: 1, Batch: 296, D Loss: 0.4065316915512085, G Loss: 2.53847599029541\n",
            "Epoch: 1, Batch: 297, D Loss: 0.4057621657848358, G Loss: 2.5437729358673096\n",
            "Epoch: 1, Batch: 298, D Loss: 0.4049053192138672, G Loss: 2.549992799758911\n",
            "Epoch: 1, Batch: 299, D Loss: 0.40399861335754395, G Loss: 2.5550880432128906\n",
            "Epoch: 1, Batch: 300, D Loss: 0.40312421321868896, G Loss: 2.558311939239502\n",
            "Epoch: 1, Batch: 301, D Loss: 0.4024500250816345, G Loss: 2.5610623359680176\n",
            "Epoch: 1, Batch: 302, D Loss: 0.40203166007995605, G Loss: 2.5634281635284424\n",
            "Epoch: 1, Batch: 303, D Loss: 0.4023018181324005, G Loss: 2.563690662384033\n",
            "Epoch: 1, Batch: 304, D Loss: 0.4027153253555298, G Loss: 2.5640268325805664\n",
            "Epoch: 1, Batch: 305, D Loss: 0.40257734060287476, G Loss: 2.5678484439849854\n",
            "Epoch: 1, Batch: 306, D Loss: 0.40226832032203674, G Loss: 2.571429967880249\n",
            "Epoch: 1, Batch: 307, D Loss: 0.4016692042350769, G Loss: 2.5734338760375977\n",
            "Epoch: 1, Batch: 308, D Loss: 0.4009402394294739, G Loss: 2.576369285583496\n",
            "Epoch: 1, Batch: 309, D Loss: 0.4001578688621521, G Loss: 2.5822389125823975\n",
            "Epoch: 1, Batch: 310, D Loss: 0.39937278628349304, G Loss: 2.5874736309051514\n",
            "Epoch: 1, Batch: 311, D Loss: 0.3986079692840576, G Loss: 2.591175079345703\n",
            "Epoch: 1, Batch: 312, D Loss: 0.3978540599346161, G Loss: 2.597111701965332\n",
            "Epoch: 1, Batch: 313, D Loss: 0.39709168672561646, G Loss: 2.605802297592163\n",
            "Epoch: 1, Batch: 314, D Loss: 0.39634668827056885, G Loss: 2.614408016204834\n",
            "Epoch: 1, Batch: 315, D Loss: 0.39555075764656067, G Loss: 2.6228177547454834\n",
            "Epoch: 1, Batch: 316, D Loss: 0.3947823643684387, G Loss: 2.6313300132751465\n",
            "Epoch: 1, Batch: 317, D Loss: 0.39396122097969055, G Loss: 2.640352487564087\n",
            "Epoch: 1, Batch: 318, D Loss: 0.3931630253791809, G Loss: 2.6504626274108887\n",
            "Epoch: 1, Batch: 319, D Loss: 0.39238592982292175, G Loss: 2.6616392135620117\n",
            "Epoch: 1, Batch: 320, D Loss: 0.3916277289390564, G Loss: 2.6717090606689453\n",
            "Epoch: 1, Batch: 321, D Loss: 0.39078307151794434, G Loss: 2.679175615310669\n",
            "Epoch: 1, Batch: 322, D Loss: 0.38999074697494507, G Loss: 2.684478759765625\n",
            "Epoch: 1, Batch: 323, D Loss: 0.3892267644405365, G Loss: 2.690502405166626\n",
            "Epoch: 1, Batch: 324, D Loss: 0.38845935463905334, G Loss: 2.699018716812134\n",
            "Epoch: 1, Batch: 325, D Loss: 0.3876858949661255, G Loss: 2.707662582397461\n",
            "Epoch: 1, Batch: 326, D Loss: 0.38689279556274414, G Loss: 2.7151107788085938\n",
            "Epoch: 1, Batch: 327, D Loss: 0.3860984444618225, G Loss: 2.7232143878936768\n",
            "Epoch: 1, Batch: 328, D Loss: 0.3853324055671692, G Loss: 2.7338929176330566\n",
            "Epoch: 1, Batch: 329, D Loss: 0.3845362663269043, G Loss: 2.7474777698516846\n",
            "Epoch: 1, Batch: 330, D Loss: 0.3837644159793854, G Loss: 2.7614529132843018\n",
            "Epoch: 1, Batch: 331, D Loss: 0.38297751545906067, G Loss: 2.773512125015259\n",
            "Epoch: 1, Batch: 332, D Loss: 0.38217484951019287, G Loss: 2.7842838764190674\n",
            "Epoch: 1, Batch: 333, D Loss: 0.38138678669929504, G Loss: 2.7932493686676025\n",
            "Epoch: 1, Batch: 334, D Loss: 0.3805832862854004, G Loss: 2.801992893218994\n",
            "Epoch: 1, Batch: 335, D Loss: 0.3797888457775116, G Loss: 2.810795307159424\n",
            "Epoch: 1, Batch: 336, D Loss: 0.3789939880371094, G Loss: 2.8194146156311035\n",
            "Epoch: 1, Batch: 337, D Loss: 0.37819114327430725, G Loss: 2.8279449939727783\n",
            "Epoch: 1, Batch: 338, D Loss: 0.37739330530166626, G Loss: 2.8344919681549072\n",
            "Epoch: 1, Batch: 339, D Loss: 0.37663641571998596, G Loss: 2.8400614261627197\n",
            "Epoch: 1, Batch: 340, D Loss: 0.37589502334594727, G Loss: 2.8457462787628174\n",
            "Epoch: 1, Batch: 341, D Loss: 0.37516987323760986, G Loss: 2.8506031036376953\n",
            "Epoch: 1, Batch: 342, D Loss: 0.3744801878929138, G Loss: 2.8554837703704834\n",
            "Epoch: 1, Batch: 343, D Loss: 0.37375307083129883, G Loss: 2.8605806827545166\n",
            "Epoch: 1, Batch: 344, D Loss: 0.373016893863678, G Loss: 2.8673675060272217\n",
            "Epoch: 1, Batch: 345, D Loss: 0.3723157048225403, G Loss: 2.875340700149536\n",
            "Epoch: 1, Batch: 346, D Loss: 0.37156033515930176, G Loss: 2.882723331451416\n",
            "Epoch: 1, Batch: 347, D Loss: 0.37086722254753113, G Loss: 2.8896586894989014\n",
            "Epoch: 1, Batch: 348, D Loss: 0.37015852332115173, G Loss: 2.898036003112793\n",
            "Epoch: 1, Batch: 349, D Loss: 0.36949458718299866, G Loss: 2.903822183609009\n",
            "Epoch: 1, Batch: 350, D Loss: 0.3689025640487671, G Loss: 2.910712718963623\n",
            "Epoch: 1, Batch: 351, D Loss: 0.36842554807662964, G Loss: 2.9210572242736816\n",
            "Epoch: 1, Batch: 352, D Loss: 0.3679397404193878, G Loss: 2.925516366958618\n",
            "Epoch: 1, Batch: 353, D Loss: 0.36725813150405884, G Loss: 2.929516553878784\n",
            "Epoch: 1, Batch: 354, D Loss: 0.36660510301589966, G Loss: 2.934807300567627\n",
            "Epoch: 1, Batch: 355, D Loss: 0.36596906185150146, G Loss: 2.9423539638519287\n",
            "Epoch: 1, Batch: 356, D Loss: 0.3652854859828949, G Loss: 2.955092191696167\n",
            "Epoch: 1, Batch: 357, D Loss: 0.36456549167633057, G Loss: 2.9734740257263184\n",
            "Epoch: 1, Batch: 358, D Loss: 0.3638606667518616, G Loss: 2.9949400424957275\n",
            "Epoch: 1, Batch: 359, D Loss: 0.3631632328033447, G Loss: 3.0171966552734375\n",
            "Epoch: 1, Batch: 360, D Loss: 0.3624739944934845, G Loss: 3.0383107662200928\n",
            "Epoch: 1, Batch: 361, D Loss: 0.3617616295814514, G Loss: 3.0579419136047363\n",
            "Epoch: 1, Batch: 362, D Loss: 0.3610529899597168, G Loss: 3.0770263671875\n",
            "Epoch: 1, Batch: 363, D Loss: 0.36035406589508057, G Loss: 3.096330165863037\n",
            "Epoch: 1, Batch: 364, D Loss: 0.3596656918525696, G Loss: 3.11650013923645\n",
            "Epoch: 1, Batch: 365, D Loss: 0.3589542806148529, G Loss: 3.137775421142578\n",
            "Epoch: 1, Batch: 366, D Loss: 0.3582807779312134, G Loss: 3.160053253173828\n",
            "Epoch: 1, Batch: 367, D Loss: 0.35759252309799194, G Loss: 3.1821351051330566\n",
            "Epoch: 1, Batch: 368, D Loss: 0.3569238781929016, G Loss: 3.2032763957977295\n",
            "Epoch: 1, Batch: 369, D Loss: 0.35623061656951904, G Loss: 3.2230112552642822\n",
            "Epoch: 1, Batch: 370, D Loss: 0.3555610179901123, G Loss: 3.2417938709259033\n",
            "Epoch: 1, Batch: 371, D Loss: 0.35486575961112976, G Loss: 3.26015305519104\n",
            "Epoch: 1, Batch: 372, D Loss: 0.35419321060180664, G Loss: 3.278475046157837\n",
            "Epoch: 1, Batch: 373, D Loss: 0.35352396965026855, G Loss: 3.2970261573791504\n",
            "Epoch: 1, Batch: 374, D Loss: 0.35284149646759033, G Loss: 3.3156704902648926\n",
            "Epoch: 1, Batch: 375, D Loss: 0.35217446088790894, G Loss: 3.3339996337890625\n",
            "Epoch: 1, Batch: 376, D Loss: 0.3515157699584961, G Loss: 3.351526975631714\n",
            "Epoch: 1, Batch: 377, D Loss: 0.350871205329895, G Loss: 3.368427038192749\n",
            "Epoch: 1, Batch: 378, D Loss: 0.35021597146987915, G Loss: 3.3844692707061768\n",
            "Epoch: 1, Batch: 379, D Loss: 0.34956467151641846, G Loss: 3.3998005390167236\n",
            "Epoch: 1, Batch: 380, D Loss: 0.34890085458755493, G Loss: 3.4144885540008545\n",
            "Epoch: 1, Batch: 381, D Loss: 0.34824734926223755, G Loss: 3.4280388355255127\n",
            "Epoch: 1, Batch: 382, D Loss: 0.3476121127605438, G Loss: 3.4408111572265625\n",
            "Epoch: 1, Batch: 383, D Loss: 0.34698039293289185, G Loss: 3.453615188598633\n",
            "Epoch: 1, Batch: 384, D Loss: 0.3463330864906311, G Loss: 3.466555595397949\n",
            "Epoch: 1, Batch: 385, D Loss: 0.3456942141056061, G Loss: 3.4791386127471924\n",
            "Epoch: 1, Batch: 386, D Loss: 0.3450719714164734, G Loss: 3.491314649581909\n",
            "Epoch: 1, Batch: 387, D Loss: 0.3444439172744751, G Loss: 3.5028085708618164\n",
            "Epoch: 1, Batch: 388, D Loss: 0.3438234329223633, G Loss: 3.5136818885803223\n",
            "Epoch: 1, Batch: 389, D Loss: 0.3432006239891052, G Loss: 3.524266004562378\n",
            "Epoch: 1, Batch: 390, D Loss: 0.3425864577293396, G Loss: 3.5339462757110596\n",
            "Epoch: 1, Batch: 391, D Loss: 0.34197235107421875, G Loss: 3.5425169467926025\n",
            "Epoch: 1, Batch: 392, D Loss: 0.34136801958084106, G Loss: 3.5512006282806396\n",
            "Epoch: 1, Batch: 393, D Loss: 0.3407699763774872, G Loss: 3.5596718788146973\n",
            "Epoch: 1, Batch: 394, D Loss: 0.3401743173599243, G Loss: 3.567084550857544\n",
            "Epoch: 1, Batch: 395, D Loss: 0.33957117795944214, G Loss: 3.57399582862854\n",
            "Epoch: 1, Batch: 396, D Loss: 0.33897584676742554, G Loss: 3.5802414417266846\n",
            "Epoch: 1, Batch: 397, D Loss: 0.33838117122650146, G Loss: 3.5859532356262207\n",
            "Epoch: 1, Batch: 398, D Loss: 0.33779317140579224, G Loss: 3.5912094116210938\n",
            "Epoch: 1, Batch: 399, D Loss: 0.3372114598751068, G Loss: 3.5962953567504883\n",
            "Epoch: 1, Batch: 400, D Loss: 0.33662521839141846, G Loss: 3.601726531982422\n",
            "Epoch: 1, Batch: 401, D Loss: 0.3360544443130493, G Loss: 3.6067636013031006\n",
            "Epoch: 1, Batch: 402, D Loss: 0.3354778289794922, G Loss: 3.610083818435669\n",
            "Epoch: 1, Batch: 403, D Loss: 0.33489513397216797, G Loss: 3.613218307495117\n",
            "Epoch: 1, Batch: 404, D Loss: 0.33432263135910034, G Loss: 3.616081714630127\n",
            "Epoch: 1, Batch: 405, D Loss: 0.33375486731529236, G Loss: 3.618114471435547\n",
            "Epoch: 1, Batch: 406, D Loss: 0.333207368850708, G Loss: 3.620798349380493\n",
            "Epoch: 1, Batch: 407, D Loss: 0.33266857266426086, G Loss: 3.6229984760284424\n",
            "Epoch: 1, Batch: 408, D Loss: 0.332131564617157, G Loss: 3.6254382133483887\n",
            "Epoch: 1, Batch: 409, D Loss: 0.331592857837677, G Loss: 3.628357410430908\n",
            "Epoch: 1, Batch: 410, D Loss: 0.3310595154762268, G Loss: 3.632648468017578\n",
            "Epoch: 1, Batch: 411, D Loss: 0.3305095434188843, G Loss: 3.6368470191955566\n",
            "Epoch: 1, Batch: 412, D Loss: 0.3299616575241089, G Loss: 3.6404659748077393\n",
            "Epoch: 1, Batch: 413, D Loss: 0.32942143082618713, G Loss: 3.64424467086792\n",
            "Epoch: 1, Batch: 414, D Loss: 0.3288920521736145, G Loss: 3.649231433868408\n",
            "Epoch: 1, Batch: 415, D Loss: 0.3283610939979553, G Loss: 3.654191017150879\n",
            "Epoch: 1, Batch: 416, D Loss: 0.3278278708457947, G Loss: 3.659982442855835\n",
            "Epoch: 1, Batch: 417, D Loss: 0.3272840976715088, G Loss: 3.666882276535034\n",
            "Epoch: 1, Batch: 418, D Loss: 0.32675960659980774, G Loss: 3.672877311706543\n",
            "Epoch: 1, Batch: 419, D Loss: 0.32622653245925903, G Loss: 3.6788437366485596\n",
            "Epoch: 1, Batch: 420, D Loss: 0.32571667432785034, G Loss: 3.6854934692382812\n",
            "Epoch: 1, Batch: 421, D Loss: 0.32519882917404175, G Loss: 3.6902496814727783\n",
            "Epoch: 1, Batch: 422, D Loss: 0.32468974590301514, G Loss: 3.6955554485321045\n",
            "Epoch: 1, Batch: 423, D Loss: 0.32417601346969604, G Loss: 3.700740337371826\n",
            "Epoch: 1, Batch: 424, D Loss: 0.323670357465744, G Loss: 3.7056963443756104\n",
            "Epoch: 1, Batch: 425, D Loss: 0.3231664299964905, G Loss: 3.7107269763946533\n",
            "Epoch: 1, Batch: 426, D Loss: 0.3226682245731354, G Loss: 3.716416597366333\n",
            "Epoch: 1, Batch: 427, D Loss: 0.32215845584869385, G Loss: 3.722822427749634\n",
            "Epoch: 1, Batch: 428, D Loss: 0.32164788246154785, G Loss: 3.7300307750701904\n",
            "Epoch: 1, Batch: 429, D Loss: 0.3211383819580078, G Loss: 3.7369284629821777\n",
            "Epoch: 1, Batch: 430, D Loss: 0.32063746452331543, G Loss: 3.7446470260620117\n",
            "Epoch: 1, Batch: 431, D Loss: 0.3201225996017456, G Loss: 3.7529053688049316\n",
            "Epoch: 1, Batch: 432, D Loss: 0.3196268379688263, G Loss: 3.7612812519073486\n",
            "Epoch: 1, Batch: 433, D Loss: 0.31912219524383545, G Loss: 3.768558979034424\n",
            "Epoch: 1, Batch: 434, D Loss: 0.3186241388320923, G Loss: 3.775209903717041\n",
            "Epoch: 1, Batch: 435, D Loss: 0.31813281774520874, G Loss: 3.781804084777832\n",
            "Epoch: 1, Batch: 436, D Loss: 0.31765085458755493, G Loss: 3.7887845039367676\n",
            "Epoch: 1, Batch: 437, D Loss: 0.31715500354766846, G Loss: 3.7962186336517334\n",
            "Epoch: 1, Batch: 438, D Loss: 0.316664457321167, G Loss: 3.8038711547851562\n",
            "Epoch: 1, Batch: 439, D Loss: 0.31618964672088623, G Loss: 3.811913013458252\n",
            "Epoch: 1, Batch: 440, D Loss: 0.31572508811950684, G Loss: 3.8203160762786865\n",
            "Epoch: 1, Batch: 441, D Loss: 0.3152397572994232, G Loss: 3.8283956050872803\n",
            "Epoch: 1, Batch: 442, D Loss: 0.31476297974586487, G Loss: 3.8362526893615723\n",
            "Epoch: 1, Batch: 443, D Loss: 0.31428301334381104, G Loss: 3.843965768814087\n",
            "Epoch: 1, Batch: 444, D Loss: 0.3137964606285095, G Loss: 3.851696014404297\n",
            "Epoch: 1, Batch: 445, D Loss: 0.3133172392845154, G Loss: 3.859574317932129\n",
            "Epoch: 1, Batch: 446, D Loss: 0.31285613775253296, G Loss: 3.8676536083221436\n",
            "Epoch: 1, Batch: 447, D Loss: 0.31239229440689087, G Loss: 3.8759925365448\n",
            "Epoch: 1, Batch: 448, D Loss: 0.3119487762451172, G Loss: 3.88454008102417\n",
            "Epoch: 1, Batch: 449, D Loss: 0.3114730417728424, G Loss: 3.8929085731506348\n",
            "Epoch: 1, Batch: 450, D Loss: 0.3109891712665558, G Loss: 3.9009673595428467\n",
            "Epoch: 1, Batch: 451, D Loss: 0.31052300333976746, G Loss: 3.908869743347168\n",
            "Epoch: 1, Batch: 452, D Loss: 0.31005942821502686, G Loss: 3.9158289432525635\n",
            "Epoch: 1, Batch: 453, D Loss: 0.30960506200790405, G Loss: 3.922569751739502\n",
            "Epoch: 1, Batch: 454, D Loss: 0.3091621398925781, G Loss: 3.9298810958862305\n",
            "Epoch: 1, Batch: 455, D Loss: 0.30870920419692993, G Loss: 3.9375128746032715\n",
            "Epoch: 1, Batch: 456, D Loss: 0.3082626461982727, G Loss: 3.945341110229492\n",
            "Epoch: 1, Batch: 457, D Loss: 0.30780526995658875, G Loss: 3.9529311656951904\n",
            "Epoch: 1, Batch: 458, D Loss: 0.3073577284812927, G Loss: 3.96036958694458\n",
            "Epoch: 1, Batch: 459, D Loss: 0.30691269040107727, G Loss: 3.9678337574005127\n",
            "Epoch: 1, Batch: 460, D Loss: 0.3064766526222229, G Loss: 3.975511074066162\n",
            "Epoch: 1, Batch: 461, D Loss: 0.30604517459869385, G Loss: 3.9835164546966553\n",
            "Epoch: 1, Batch: 462, D Loss: 0.30559849739074707, G Loss: 3.991530179977417\n",
            "Epoch: 1, Batch: 463, D Loss: 0.30515748262405396, G Loss: 3.999330520629883\n",
            "Epoch: 1, Batch: 464, D Loss: 0.3047226667404175, G Loss: 4.007169246673584\n",
            "Epoch: 1, Batch: 465, D Loss: 0.304277241230011, G Loss: 4.014961242675781\n",
            "Epoch: 1, Batch: 466, D Loss: 0.3038472831249237, G Loss: 4.0228142738342285\n",
            "Epoch: 1, Batch: 467, D Loss: 0.30340540409088135, G Loss: 4.030817031860352\n",
            "Epoch: 2, Batch: 0, D Loss: 0.3029581606388092, G Loss: 4.0387444496154785\n",
            "Epoch: 2, Batch: 1, D Loss: 0.302517831325531, G Loss: 4.045464992523193\n",
            "Epoch: 2, Batch: 2, D Loss: 0.3020927906036377, G Loss: 4.051937103271484\n",
            "Epoch: 2, Batch: 3, D Loss: 0.3016558885574341, G Loss: 4.058509826660156\n",
            "Epoch: 2, Batch: 4, D Loss: 0.30123889446258545, G Loss: 4.065389633178711\n",
            "Epoch: 2, Batch: 5, D Loss: 0.30082830786705017, G Loss: 4.072556495666504\n",
            "Epoch: 2, Batch: 6, D Loss: 0.30039864778518677, G Loss: 4.078846454620361\n",
            "Epoch: 2, Batch: 7, D Loss: 0.29997140169143677, G Loss: 4.08495569229126\n",
            "Epoch: 2, Batch: 8, D Loss: 0.2995602488517761, G Loss: 4.091048717498779\n",
            "Epoch: 2, Batch: 9, D Loss: 0.2991432547569275, G Loss: 4.097268104553223\n",
            "Epoch: 2, Batch: 10, D Loss: 0.2987235188484192, G Loss: 4.103636264801025\n",
            "Epoch: 2, Batch: 11, D Loss: 0.2983137369155884, G Loss: 4.11035680770874\n",
            "Epoch: 2, Batch: 12, D Loss: 0.2978954315185547, G Loss: 4.117294788360596\n",
            "Epoch: 2, Batch: 13, D Loss: 0.2974775433540344, G Loss: 4.1244049072265625\n",
            "Epoch: 2, Batch: 14, D Loss: 0.29706263542175293, G Loss: 4.131669521331787\n",
            "Epoch: 2, Batch: 15, D Loss: 0.29665127396583557, G Loss: 4.1390275955200195\n",
            "Epoch: 2, Batch: 16, D Loss: 0.29625606536865234, G Loss: 4.146631717681885\n",
            "Epoch: 2, Batch: 17, D Loss: 0.2958541810512543, G Loss: 4.154411792755127\n",
            "Epoch: 2, Batch: 18, D Loss: 0.2954670488834381, G Loss: 4.162374019622803\n",
            "Epoch: 2, Batch: 19, D Loss: 0.2950827479362488, G Loss: 4.170346260070801\n",
            "Epoch: 2, Batch: 20, D Loss: 0.29468223452568054, G Loss: 4.178177356719971\n",
            "Epoch: 2, Batch: 21, D Loss: 0.2942839562892914, G Loss: 4.185906887054443\n",
            "Epoch: 2, Batch: 22, D Loss: 0.29391390085220337, G Loss: 4.193819046020508\n",
            "Epoch: 2, Batch: 23, D Loss: 0.2935329079627991, G Loss: 4.2017107009887695\n",
            "Epoch: 2, Batch: 24, D Loss: 0.2931361794471741, G Loss: 4.209310054779053\n",
            "Epoch: 2, Batch: 25, D Loss: 0.29275065660476685, G Loss: 4.216286659240723\n",
            "Epoch: 2, Batch: 26, D Loss: 0.2923743426799774, G Loss: 4.223026275634766\n",
            "Epoch: 2, Batch: 27, D Loss: 0.2919955253601074, G Loss: 4.229873180389404\n",
            "Epoch: 2, Batch: 28, D Loss: 0.2916131019592285, G Loss: 4.236754894256592\n",
            "Epoch: 2, Batch: 29, D Loss: 0.29123175144195557, G Loss: 4.24367618560791\n",
            "Epoch: 2, Batch: 30, D Loss: 0.2908380627632141, G Loss: 4.250573635101318\n",
            "Epoch: 2, Batch: 31, D Loss: 0.2904612720012665, G Loss: 4.257569789886475\n",
            "Epoch: 2, Batch: 32, D Loss: 0.29007548093795776, G Loss: 4.264588832855225\n",
            "Epoch: 2, Batch: 33, D Loss: 0.28970760107040405, G Loss: 4.271800518035889\n",
            "Epoch: 2, Batch: 34, D Loss: 0.2893267273902893, G Loss: 4.279069900512695\n",
            "Epoch: 2, Batch: 35, D Loss: 0.28895264863967896, G Loss: 4.286230564117432\n",
            "Epoch: 2, Batch: 36, D Loss: 0.288562536239624, G Loss: 4.292644023895264\n",
            "Epoch: 2, Batch: 37, D Loss: 0.28818854689598083, G Loss: 4.298984527587891\n",
            "Epoch: 2, Batch: 38, D Loss: 0.2878159284591675, G Loss: 4.305412292480469\n",
            "Epoch: 2, Batch: 39, D Loss: 0.2874407172203064, G Loss: 4.311971664428711\n",
            "Epoch: 2, Batch: 40, D Loss: 0.2870723605155945, G Loss: 4.318731307983398\n",
            "Epoch: 2, Batch: 41, D Loss: 0.2867140769958496, G Loss: 4.325530052185059\n",
            "Epoch: 2, Batch: 42, D Loss: 0.28634434938430786, G Loss: 4.3313889503479\n",
            "Epoch: 2, Batch: 43, D Loss: 0.2859852910041809, G Loss: 4.33596134185791\n",
            "Epoch: 2, Batch: 44, D Loss: 0.2856251001358032, G Loss: 4.340574741363525\n",
            "Epoch: 2, Batch: 45, D Loss: 0.285266637802124, G Loss: 4.3447160720825195\n",
            "Epoch: 2, Batch: 46, D Loss: 0.284909188747406, G Loss: 4.348937034606934\n",
            "Epoch: 2, Batch: 47, D Loss: 0.2845708131790161, G Loss: 4.353662967681885\n",
            "Epoch: 2, Batch: 48, D Loss: 0.28422433137893677, G Loss: 4.358880519866943\n",
            "Epoch: 2, Batch: 49, D Loss: 0.28386133909225464, G Loss: 4.364358425140381\n",
            "Epoch: 2, Batch: 50, D Loss: 0.28350624442100525, G Loss: 4.369950294494629\n",
            "Epoch: 2, Batch: 51, D Loss: 0.2831431031227112, G Loss: 4.37563943862915\n",
            "Epoch: 2, Batch: 52, D Loss: 0.2828011214733124, G Loss: 4.381502151489258\n",
            "Epoch: 2, Batch: 53, D Loss: 0.2824631631374359, G Loss: 4.387740135192871\n",
            "Epoch: 2, Batch: 54, D Loss: 0.2821178436279297, G Loss: 4.394321918487549\n",
            "Epoch: 2, Batch: 55, D Loss: 0.2817659378051758, G Loss: 4.400985240936279\n",
            "Epoch: 2, Batch: 56, D Loss: 0.2814221978187561, G Loss: 4.407598972320557\n",
            "Epoch: 2, Batch: 57, D Loss: 0.28107795119285583, G Loss: 4.414216041564941\n",
            "Epoch: 2, Batch: 58, D Loss: 0.28073650598526, G Loss: 4.420910835266113\n",
            "Epoch: 2, Batch: 59, D Loss: 0.2803959846496582, G Loss: 4.427799224853516\n",
            "Epoch: 2, Batch: 60, D Loss: 0.2800626754760742, G Loss: 4.434906959533691\n",
            "Epoch: 2, Batch: 61, D Loss: 0.27972695231437683, G Loss: 4.442160129547119\n",
            "Epoch: 2, Batch: 62, D Loss: 0.2793775796890259, G Loss: 4.449188232421875\n",
            "Epoch: 2, Batch: 63, D Loss: 0.27904951572418213, G Loss: 4.456141471862793\n",
            "Epoch: 2, Batch: 64, D Loss: 0.2787168622016907, G Loss: 4.462464809417725\n",
            "Epoch: 2, Batch: 65, D Loss: 0.27838587760925293, G Loss: 4.468476295471191\n",
            "Epoch: 2, Batch: 66, D Loss: 0.278059720993042, G Loss: 4.4747161865234375\n",
            "Epoch: 2, Batch: 67, D Loss: 0.27773523330688477, G Loss: 4.481054306030273\n",
            "Epoch: 2, Batch: 68, D Loss: 0.27740272879600525, G Loss: 4.487341403961182\n",
            "Epoch: 2, Batch: 69, D Loss: 0.27707213163375854, G Loss: 4.49348783493042\n",
            "Epoch: 2, Batch: 70, D Loss: 0.2767326831817627, G Loss: 4.499451160430908\n",
            "Epoch: 2, Batch: 71, D Loss: 0.2764078378677368, G Loss: 4.505398273468018\n",
            "Epoch: 2, Batch: 72, D Loss: 0.27608662843704224, G Loss: 4.511571407318115\n",
            "Epoch: 2, Batch: 73, D Loss: 0.2757633328437805, G Loss: 4.517977714538574\n",
            "Epoch: 2, Batch: 74, D Loss: 0.27543458342552185, G Loss: 4.524585247039795\n",
            "Epoch: 2, Batch: 75, D Loss: 0.2751115560531616, G Loss: 4.531213283538818\n",
            "Epoch: 2, Batch: 76, D Loss: 0.2747819423675537, G Loss: 4.537321090698242\n",
            "Epoch: 2, Batch: 77, D Loss: 0.27446436882019043, G Loss: 4.543261528015137\n",
            "Epoch: 2, Batch: 78, D Loss: 0.27415141463279724, G Loss: 4.549086093902588\n",
            "Epoch: 2, Batch: 79, D Loss: 0.2738332748413086, G Loss: 4.554239749908447\n",
            "Epoch: 2, Batch: 80, D Loss: 0.27351152896881104, G Loss: 4.559535980224609\n",
            "Epoch: 2, Batch: 81, D Loss: 0.2731969952583313, G Loss: 4.565000534057617\n",
            "Epoch: 2, Batch: 82, D Loss: 0.2728766202926636, G Loss: 4.570624351501465\n",
            "Epoch: 2, Batch: 83, D Loss: 0.2725633680820465, G Loss: 4.576301097869873\n",
            "Epoch: 2, Batch: 84, D Loss: 0.27225539088249207, G Loss: 4.582096576690674\n",
            "Epoch: 2, Batch: 85, D Loss: 0.27194249629974365, G Loss: 4.5878472328186035\n",
            "Epoch: 2, Batch: 86, D Loss: 0.2716299891471863, G Loss: 4.593352317810059\n",
            "Epoch: 2, Batch: 87, D Loss: 0.27132654190063477, G Loss: 4.598834037780762\n",
            "Epoch: 2, Batch: 88, D Loss: 0.27101078629493713, G Loss: 4.603950500488281\n",
            "Epoch: 2, Batch: 89, D Loss: 0.2706994414329529, G Loss: 4.608926296234131\n",
            "Epoch: 2, Batch: 90, D Loss: 0.270397424697876, G Loss: 4.614096641540527\n",
            "Epoch: 2, Batch: 91, D Loss: 0.2700819969177246, G Loss: 4.619356632232666\n",
            "Epoch: 2, Batch: 92, D Loss: 0.2697718143463135, G Loss: 4.624649524688721\n",
            "Epoch: 2, Batch: 93, D Loss: 0.26946932077407837, G Loss: 4.630046844482422\n",
            "Epoch: 2, Batch: 94, D Loss: 0.26918062567710876, G Loss: 4.6357927322387695\n",
            "Epoch: 2, Batch: 95, D Loss: 0.26887792348861694, G Loss: 4.640974998474121\n",
            "Epoch: 2, Batch: 96, D Loss: 0.26858848333358765, G Loss: 4.646020889282227\n",
            "Epoch: 2, Batch: 97, D Loss: 0.2682996392250061, G Loss: 4.651264190673828\n",
            "Epoch: 2, Batch: 98, D Loss: 0.2680056691169739, G Loss: 4.656543731689453\n",
            "Epoch: 2, Batch: 99, D Loss: 0.2677093744277954, G Loss: 4.661694049835205\n",
            "Epoch: 2, Batch: 100, D Loss: 0.2674175500869751, G Loss: 4.666806697845459\n",
            "Epoch: 2, Batch: 101, D Loss: 0.26712581515312195, G Loss: 4.672019958496094\n",
            "Epoch: 2, Batch: 102, D Loss: 0.2668277621269226, G Loss: 4.6774091720581055\n",
            "Epoch: 2, Batch: 103, D Loss: 0.2665294408798218, G Loss: 4.682961463928223\n",
            "Epoch: 2, Batch: 104, D Loss: 0.2662454843521118, G Loss: 4.688634395599365\n",
            "Epoch: 2, Batch: 105, D Loss: 0.26595455408096313, G Loss: 4.694347381591797\n",
            "Epoch: 2, Batch: 106, D Loss: 0.2656736373901367, G Loss: 4.700170040130615\n",
            "Epoch: 2, Batch: 107, D Loss: 0.2653810977935791, G Loss: 4.705953121185303\n",
            "Epoch: 2, Batch: 108, D Loss: 0.2650941014289856, G Loss: 4.711556434631348\n",
            "Epoch: 2, Batch: 109, D Loss: 0.2647998631000519, G Loss: 4.716719150543213\n",
            "Epoch: 2, Batch: 110, D Loss: 0.2645106315612793, G Loss: 4.721707344055176\n",
            "Epoch: 2, Batch: 111, D Loss: 0.26421740651130676, G Loss: 4.726754188537598\n",
            "Epoch: 2, Batch: 112, D Loss: 0.26394838094711304, G Loss: 4.732132434844971\n",
            "Epoch: 2, Batch: 113, D Loss: 0.2636619210243225, G Loss: 4.737801551818848\n",
            "Epoch: 2, Batch: 114, D Loss: 0.26338693499565125, G Loss: 4.74360990524292\n",
            "Epoch: 2, Batch: 115, D Loss: 0.2631203532218933, G Loss: 4.749440670013428\n",
            "Epoch: 2, Batch: 116, D Loss: 0.2628427743911743, G Loss: 4.755178451538086\n",
            "Epoch: 2, Batch: 117, D Loss: 0.2625563144683838, G Loss: 4.760668754577637\n",
            "Epoch: 2, Batch: 118, D Loss: 0.2622835338115692, G Loss: 4.766181468963623\n",
            "Epoch: 2, Batch: 119, D Loss: 0.2620110511779785, G Loss: 4.77141809463501\n",
            "Epoch: 2, Batch: 120, D Loss: 0.2617294490337372, G Loss: 4.776564121246338\n",
            "Epoch: 2, Batch: 121, D Loss: 0.261463463306427, G Loss: 4.781932830810547\n",
            "Epoch: 2, Batch: 122, D Loss: 0.26119065284729004, G Loss: 4.7871994972229\n",
            "Epoch: 2, Batch: 123, D Loss: 0.2609212398529053, G Loss: 4.792248249053955\n",
            "Epoch: 2, Batch: 124, D Loss: 0.26064759492874146, G Loss: 4.796665668487549\n",
            "Epoch: 2, Batch: 125, D Loss: 0.26038604974746704, G Loss: 4.800449371337891\n",
            "Epoch: 2, Batch: 126, D Loss: 0.2601155638694763, G Loss: 4.80445671081543\n",
            "Epoch: 2, Batch: 127, D Loss: 0.2598484456539154, G Loss: 4.808722972869873\n",
            "Epoch: 2, Batch: 128, D Loss: 0.25957924127578735, G Loss: 4.81310510635376\n",
            "Epoch: 2, Batch: 129, D Loss: 0.25932034850120544, G Loss: 4.817553520202637\n",
            "Epoch: 2, Batch: 130, D Loss: 0.25905489921569824, G Loss: 4.8215508460998535\n",
            "Epoch: 2, Batch: 131, D Loss: 0.25879162549972534, G Loss: 4.825616359710693\n",
            "Epoch: 2, Batch: 132, D Loss: 0.25853025913238525, G Loss: 4.829898357391357\n",
            "Epoch: 2, Batch: 133, D Loss: 0.25827455520629883, G Loss: 4.834010601043701\n",
            "Epoch: 2, Batch: 134, D Loss: 0.258015513420105, G Loss: 4.8372087478637695\n",
            "Epoch: 2, Batch: 135, D Loss: 0.2577563524246216, G Loss: 4.840335845947266\n",
            "Epoch: 2, Batch: 136, D Loss: 0.2575036883354187, G Loss: 4.843573570251465\n",
            "Epoch: 2, Batch: 137, D Loss: 0.2572471499443054, G Loss: 4.846446990966797\n",
            "Epoch: 2, Batch: 138, D Loss: 0.25698813796043396, G Loss: 4.849711894989014\n",
            "Epoch: 2, Batch: 139, D Loss: 0.25673025846481323, G Loss: 4.853316783905029\n",
            "Epoch: 2, Batch: 140, D Loss: 0.25646576285362244, G Loss: 4.8569440841674805\n",
            "Epoch: 2, Batch: 141, D Loss: 0.2562023997306824, G Loss: 4.860479354858398\n",
            "Epoch: 2, Batch: 142, D Loss: 0.25595152378082275, G Loss: 4.864287376403809\n",
            "Epoch: 2, Batch: 143, D Loss: 0.25569283962249756, G Loss: 4.868473052978516\n",
            "Epoch: 2, Batch: 144, D Loss: 0.25544124841690063, G Loss: 4.873007297515869\n",
            "Epoch: 2, Batch: 145, D Loss: 0.25517982244491577, G Loss: 4.877622127532959\n",
            "Epoch: 2, Batch: 146, D Loss: 0.25492849946022034, G Loss: 4.882316589355469\n",
            "Epoch: 2, Batch: 147, D Loss: 0.25468116998672485, G Loss: 4.887190818786621\n",
            "Epoch: 2, Batch: 148, D Loss: 0.2544294595718384, G Loss: 4.892134666442871\n",
            "Epoch: 2, Batch: 149, D Loss: 0.2541773319244385, G Loss: 4.895699977874756\n",
            "Epoch: 2, Batch: 150, D Loss: 0.25393253564834595, G Loss: 4.898076057434082\n",
            "Epoch: 2, Batch: 151, D Loss: 0.25368940830230713, G Loss: 4.900035381317139\n",
            "Epoch: 2, Batch: 152, D Loss: 0.2534523606300354, G Loss: 4.901413917541504\n",
            "Epoch: 2, Batch: 153, D Loss: 0.25321924686431885, G Loss: 4.902889251708984\n",
            "Epoch: 2, Batch: 154, D Loss: 0.25298792123794556, G Loss: 4.904821872711182\n",
            "Epoch: 2, Batch: 155, D Loss: 0.2527460753917694, G Loss: 4.906914710998535\n",
            "Epoch: 2, Batch: 156, D Loss: 0.25249791145324707, G Loss: 4.908961296081543\n",
            "Epoch: 2, Batch: 157, D Loss: 0.2522597312927246, G Loss: 4.9105634689331055\n",
            "Epoch: 2, Batch: 158, D Loss: 0.25202617049217224, G Loss: 4.912776947021484\n",
            "Epoch: 2, Batch: 159, D Loss: 0.25178614258766174, G Loss: 4.91562557220459\n",
            "Epoch: 2, Batch: 160, D Loss: 0.25155141949653625, G Loss: 4.918774127960205\n",
            "Epoch: 2, Batch: 161, D Loss: 0.25130176544189453, G Loss: 4.9215989112854\n",
            "Epoch: 2, Batch: 162, D Loss: 0.2510679364204407, G Loss: 4.9244279861450195\n",
            "Epoch: 2, Batch: 163, D Loss: 0.25083309412002563, G Loss: 4.926855087280273\n",
            "Epoch: 2, Batch: 164, D Loss: 0.2505921721458435, G Loss: 4.9295454025268555\n",
            "Epoch: 2, Batch: 165, D Loss: 0.25035881996154785, G Loss: 4.932557582855225\n",
            "Epoch: 2, Batch: 166, D Loss: 0.25013262033462524, G Loss: 4.935645580291748\n",
            "Epoch: 2, Batch: 167, D Loss: 0.24989405274391174, G Loss: 4.9388813972473145\n",
            "Epoch: 2, Batch: 168, D Loss: 0.249667689204216, G Loss: 4.942559719085693\n",
            "Epoch: 2, Batch: 169, D Loss: 0.2494359016418457, G Loss: 4.946274757385254\n",
            "Epoch: 2, Batch: 170, D Loss: 0.24920320510864258, G Loss: 4.949196815490723\n",
            "Epoch: 2, Batch: 171, D Loss: 0.248964324593544, G Loss: 4.95196008682251\n",
            "Epoch: 2, Batch: 172, D Loss: 0.2487366795539856, G Loss: 4.954623222351074\n",
            "Epoch: 2, Batch: 173, D Loss: 0.2485107183456421, G Loss: 4.957085132598877\n",
            "Epoch: 2, Batch: 174, D Loss: 0.24828220903873444, G Loss: 4.959420204162598\n",
            "Epoch: 2, Batch: 175, D Loss: 0.2480546534061432, G Loss: 4.961874485015869\n",
            "Epoch: 2, Batch: 176, D Loss: 0.24782659113407135, G Loss: 4.964190483093262\n",
            "Epoch: 2, Batch: 177, D Loss: 0.2476036697626114, G Loss: 4.966504096984863\n",
            "Epoch: 2, Batch: 178, D Loss: 0.2473839670419693, G Loss: 4.968255519866943\n",
            "Epoch: 2, Batch: 179, D Loss: 0.2471640259027481, G Loss: 4.969953536987305\n",
            "Epoch: 2, Batch: 180, D Loss: 0.24694103002548218, G Loss: 4.972071647644043\n",
            "Epoch: 2, Batch: 181, D Loss: 0.24671310186386108, G Loss: 4.974497318267822\n",
            "Epoch: 2, Batch: 182, D Loss: 0.24649757146835327, G Loss: 4.977375030517578\n",
            "Epoch: 2, Batch: 183, D Loss: 0.24627938866615295, G Loss: 4.980498790740967\n",
            "Epoch: 2, Batch: 184, D Loss: 0.24605995416641235, G Loss: 4.983837604522705\n",
            "Epoch: 2, Batch: 185, D Loss: 0.2458435297012329, G Loss: 4.987135887145996\n",
            "Epoch: 2, Batch: 186, D Loss: 0.2456274926662445, G Loss: 4.990470886230469\n",
            "Epoch: 2, Batch: 187, D Loss: 0.24540120363235474, G Loss: 4.993681907653809\n",
            "Epoch: 2, Batch: 188, D Loss: 0.24517613649368286, G Loss: 4.996829509735107\n",
            "Epoch: 2, Batch: 189, D Loss: 0.24496175348758698, G Loss: 5.000272274017334\n",
            "Epoch: 2, Batch: 190, D Loss: 0.24473556876182556, G Loss: 5.003596782684326\n",
            "Epoch: 2, Batch: 191, D Loss: 0.244523823261261, G Loss: 5.0059709548950195\n",
            "Epoch: 2, Batch: 192, D Loss: 0.2443120777606964, G Loss: 5.007437705993652\n",
            "Epoch: 2, Batch: 193, D Loss: 0.24409553408622742, G Loss: 5.009074687957764\n",
            "Epoch: 2, Batch: 194, D Loss: 0.2438783347606659, G Loss: 5.010871887207031\n",
            "Epoch: 2, Batch: 195, D Loss: 0.24365957081317902, G Loss: 5.012718677520752\n",
            "Epoch: 2, Batch: 196, D Loss: 0.24343740940093994, G Loss: 5.014944553375244\n",
            "Epoch: 2, Batch: 197, D Loss: 0.24323134124279022, G Loss: 5.017816066741943\n",
            "Epoch: 2, Batch: 198, D Loss: 0.24302175641059875, G Loss: 5.021121501922607\n",
            "Epoch: 2, Batch: 199, D Loss: 0.24280697107315063, G Loss: 5.024345874786377\n",
            "Epoch: 2, Batch: 200, D Loss: 0.24259409308433533, G Loss: 5.027443885803223\n",
            "Epoch: 2, Batch: 201, D Loss: 0.24237576127052307, G Loss: 5.0304670333862305\n",
            "Epoch: 2, Batch: 202, D Loss: 0.24216832220554352, G Loss: 5.033630847930908\n",
            "Epoch: 2, Batch: 203, D Loss: 0.24195954203605652, G Loss: 5.0371832847595215\n",
            "Epoch: 2, Batch: 204, D Loss: 0.24175503849983215, G Loss: 5.041084289550781\n",
            "Epoch: 2, Batch: 205, D Loss: 0.24155473709106445, G Loss: 5.044516563415527\n",
            "Epoch: 2, Batch: 206, D Loss: 0.24135001003742218, G Loss: 5.047586441040039\n",
            "Epoch: 2, Batch: 207, D Loss: 0.24115489423274994, G Loss: 5.05049991607666\n",
            "Epoch: 2, Batch: 208, D Loss: 0.24094554781913757, G Loss: 5.052553176879883\n",
            "Epoch: 2, Batch: 209, D Loss: 0.24074238538742065, G Loss: 5.054687976837158\n",
            "Epoch: 2, Batch: 210, D Loss: 0.2405361831188202, G Loss: 5.057272911071777\n",
            "Epoch: 2, Batch: 211, D Loss: 0.2403368204832077, G Loss: 5.060190200805664\n",
            "Epoch: 2, Batch: 212, D Loss: 0.2401357889175415, G Loss: 5.063281536102295\n",
            "Epoch: 2, Batch: 213, D Loss: 0.23993466794490814, G Loss: 5.066376209259033\n",
            "Epoch: 2, Batch: 214, D Loss: 0.23972338438034058, G Loss: 5.069272518157959\n",
            "Epoch: 2, Batch: 215, D Loss: 0.2395244836807251, G Loss: 5.072410583496094\n",
            "Epoch: 2, Batch: 216, D Loss: 0.23931586742401123, G Loss: 5.07585334777832\n",
            "Epoch: 2, Batch: 217, D Loss: 0.23910406231880188, G Loss: 5.079494953155518\n",
            "Epoch: 2, Batch: 218, D Loss: 0.23890581727027893, G Loss: 5.083347320556641\n",
            "Epoch: 2, Batch: 219, D Loss: 0.23870566487312317, G Loss: 5.087386131286621\n",
            "Epoch: 2, Batch: 220, D Loss: 0.23849749565124512, G Loss: 5.091449737548828\n",
            "Epoch: 2, Batch: 221, D Loss: 0.23830121755599976, G Loss: 5.095530986785889\n",
            "Epoch: 2, Batch: 222, D Loss: 0.23811447620391846, G Loss: 5.099809646606445\n",
            "Epoch: 2, Batch: 223, D Loss: 0.23791663348674774, G Loss: 5.1041412353515625\n",
            "Epoch: 2, Batch: 224, D Loss: 0.23772118985652924, G Loss: 5.108433723449707\n",
            "Epoch: 2, Batch: 225, D Loss: 0.23751549422740936, G Loss: 5.112427234649658\n",
            "Epoch: 2, Batch: 226, D Loss: 0.23732955753803253, G Loss: 5.116547107696533\n",
            "Epoch: 2, Batch: 227, D Loss: 0.23713314533233643, G Loss: 5.120870590209961\n",
            "Epoch: 2, Batch: 228, D Loss: 0.23693326115608215, G Loss: 5.124845504760742\n",
            "Epoch: 2, Batch: 229, D Loss: 0.23674169182777405, G Loss: 5.128661632537842\n",
            "Epoch: 2, Batch: 230, D Loss: 0.2365422248840332, G Loss: 5.1322550773620605\n",
            "Epoch: 2, Batch: 231, D Loss: 0.23636005818843842, G Loss: 5.135843276977539\n",
            "Epoch: 2, Batch: 232, D Loss: 0.23616304993629456, G Loss: 5.139493942260742\n",
            "Epoch: 2, Batch: 233, D Loss: 0.23598220944404602, G Loss: 5.143357276916504\n",
            "Epoch: 2, Batch: 234, D Loss: 0.23579205572605133, G Loss: 5.146866798400879\n",
            "Epoch: 2, Batch: 235, D Loss: 0.2356012761592865, G Loss: 5.150160312652588\n",
            "Epoch: 2, Batch: 236, D Loss: 0.23540395498275757, G Loss: 5.153322219848633\n",
            "Epoch: 2, Batch: 237, D Loss: 0.23521094024181366, G Loss: 5.156381130218506\n",
            "Epoch: 2, Batch: 238, D Loss: 0.235020712018013, G Loss: 5.159580707550049\n",
            "Epoch: 2, Batch: 239, D Loss: 0.23483189940452576, G Loss: 5.162999153137207\n",
            "Epoch: 2, Batch: 240, D Loss: 0.23464235663414001, G Loss: 5.166574478149414\n",
            "Epoch: 2, Batch: 241, D Loss: 0.23445791006088257, G Loss: 5.17024564743042\n",
            "Epoch: 2, Batch: 242, D Loss: 0.23426319658756256, G Loss: 5.1735758781433105\n",
            "Epoch: 2, Batch: 243, D Loss: 0.23408447206020355, G Loss: 5.176835536956787\n",
            "Epoch: 2, Batch: 244, D Loss: 0.2338913381099701, G Loss: 5.1801934242248535\n",
            "Epoch: 2, Batch: 245, D Loss: 0.23370784521102905, G Loss: 5.183831214904785\n",
            "Epoch: 2, Batch: 246, D Loss: 0.23352348804473877, G Loss: 5.1874847412109375\n",
            "Epoch: 2, Batch: 247, D Loss: 0.23333728313446045, G Loss: 5.190976619720459\n",
            "Epoch: 2, Batch: 248, D Loss: 0.23314931988716125, G Loss: 5.194370746612549\n",
            "Epoch: 2, Batch: 249, D Loss: 0.23295582830905914, G Loss: 5.197751998901367\n",
            "Epoch: 2, Batch: 250, D Loss: 0.23277774453163147, G Loss: 5.201419353485107\n",
            "Epoch: 2, Batch: 251, D Loss: 0.23259267210960388, G Loss: 5.205276966094971\n",
            "Epoch: 2, Batch: 252, D Loss: 0.23240000009536743, G Loss: 5.209050178527832\n",
            "Epoch: 2, Batch: 253, D Loss: 0.23221002519130707, G Loss: 5.212705612182617\n",
            "Epoch: 2, Batch: 254, D Loss: 0.23203988373279572, G Loss: 5.216519355773926\n",
            "Epoch: 2, Batch: 255, D Loss: 0.23185470700263977, G Loss: 5.220345497131348\n",
            "Epoch: 2, Batch: 256, D Loss: 0.23167768120765686, G Loss: 5.224338531494141\n",
            "Epoch: 2, Batch: 257, D Loss: 0.23149007558822632, G Loss: 5.22829532623291\n",
            "Epoch: 2, Batch: 258, D Loss: 0.2313070446252823, G Loss: 5.232100486755371\n",
            "Epoch: 2, Batch: 259, D Loss: 0.23112043738365173, G Loss: 5.23565149307251\n",
            "Epoch: 2, Batch: 260, D Loss: 0.23095017671585083, G Loss: 5.239359378814697\n",
            "Epoch: 2, Batch: 261, D Loss: 0.2307802140712738, G Loss: 5.243508338928223\n",
            "Epoch: 2, Batch: 262, D Loss: 0.23060011863708496, G Loss: 5.247864246368408\n",
            "Epoch: 2, Batch: 263, D Loss: 0.23042896389961243, G Loss: 5.252097129821777\n",
            "Epoch: 2, Batch: 264, D Loss: 0.23024260997772217, G Loss: 5.255938529968262\n",
            "Epoch: 2, Batch: 265, D Loss: 0.23006753623485565, G Loss: 5.259632110595703\n",
            "Epoch: 2, Batch: 266, D Loss: 0.22989818453788757, G Loss: 5.263528347015381\n",
            "Epoch: 2, Batch: 267, D Loss: 0.22972257435321808, G Loss: 5.267749786376953\n",
            "Epoch: 2, Batch: 268, D Loss: 0.22953778505325317, G Loss: 5.272080421447754\n",
            "Epoch: 2, Batch: 269, D Loss: 0.22936688363552094, G Loss: 5.276392936706543\n",
            "Epoch: 2, Batch: 270, D Loss: 0.22919347882270813, G Loss: 5.280490398406982\n",
            "Epoch: 2, Batch: 271, D Loss: 0.22900788486003876, G Loss: 5.284033298492432\n",
            "Epoch: 2, Batch: 272, D Loss: 0.22884082794189453, G Loss: 5.287561416625977\n",
            "Epoch: 2, Batch: 273, D Loss: 0.2286691963672638, G Loss: 5.291311264038086\n",
            "Epoch: 2, Batch: 274, D Loss: 0.22849763929843903, G Loss: 5.295304298400879\n",
            "Epoch: 2, Batch: 275, D Loss: 0.22832566499710083, G Loss: 5.299422264099121\n",
            "Epoch: 2, Batch: 276, D Loss: 0.22815050184726715, G Loss: 5.303433895111084\n",
            "Epoch: 2, Batch: 277, D Loss: 0.22797182202339172, G Loss: 5.307077407836914\n",
            "Epoch: 2, Batch: 278, D Loss: 0.22780609130859375, G Loss: 5.310698986053467\n",
            "Epoch: 2, Batch: 279, D Loss: 0.22763511538505554, G Loss: 5.314517498016357\n",
            "Epoch: 2, Batch: 280, D Loss: 0.22745704650878906, G Loss: 5.318551540374756\n",
            "Epoch: 2, Batch: 281, D Loss: 0.22729037702083588, G Loss: 5.3228325843811035\n",
            "Epoch: 2, Batch: 282, D Loss: 0.227121040225029, G Loss: 5.327110290527344\n",
            "Epoch: 2, Batch: 283, D Loss: 0.22695913910865784, G Loss: 5.331294059753418\n",
            "Epoch: 2, Batch: 284, D Loss: 0.22679127752780914, G Loss: 5.335205554962158\n",
            "Epoch: 2, Batch: 285, D Loss: 0.22663220763206482, G Loss: 5.339105606079102\n",
            "Epoch: 2, Batch: 286, D Loss: 0.2264707088470459, G Loss: 5.343212604522705\n",
            "Epoch: 2, Batch: 287, D Loss: 0.2263132780790329, G Loss: 5.347569465637207\n",
            "Epoch: 2, Batch: 288, D Loss: 0.22614896297454834, G Loss: 5.351963043212891\n",
            "Epoch: 2, Batch: 289, D Loss: 0.22598251700401306, G Loss: 5.3561530113220215\n",
            "Epoch: 2, Batch: 290, D Loss: 0.22581735253334045, G Loss: 5.360076427459717\n",
            "Epoch: 2, Batch: 291, D Loss: 0.2256530374288559, G Loss: 5.363819599151611\n",
            "Epoch: 2, Batch: 292, D Loss: 0.22550714015960693, G Loss: 5.367883205413818\n",
            "Epoch: 2, Batch: 293, D Loss: 0.22534331679344177, G Loss: 5.372021198272705\n",
            "Epoch: 2, Batch: 294, D Loss: 0.22518280148506165, G Loss: 5.376107692718506\n",
            "Epoch: 2, Batch: 295, D Loss: 0.2250152975320816, G Loss: 5.3800578117370605\n",
            "Epoch: 2, Batch: 296, D Loss: 0.2248527705669403, G Loss: 5.383921146392822\n",
            "Epoch: 2, Batch: 297, D Loss: 0.22468355298042297, G Loss: 5.387782096862793\n",
            "Epoch: 2, Batch: 298, D Loss: 0.22453302145004272, G Loss: 5.391912937164307\n",
            "Epoch: 2, Batch: 299, D Loss: 0.22437766194343567, G Loss: 5.396255970001221\n",
            "Epoch: 2, Batch: 300, D Loss: 0.2242129147052765, G Loss: 5.400442600250244\n",
            "Epoch: 2, Batch: 301, D Loss: 0.2240506112575531, G Loss: 5.404395580291748\n",
            "Epoch: 2, Batch: 302, D Loss: 0.22389577329158783, G Loss: 5.408290386199951\n",
            "Epoch: 2, Batch: 303, D Loss: 0.22372369468212128, G Loss: 5.412141799926758\n",
            "Epoch: 2, Batch: 304, D Loss: 0.22356289625167847, G Loss: 5.416014194488525\n",
            "Epoch: 2, Batch: 305, D Loss: 0.22339928150177002, G Loss: 5.419258117675781\n",
            "Epoch: 2, Batch: 306, D Loss: 0.22324156761169434, G Loss: 5.422597885131836\n",
            "Epoch: 2, Batch: 307, D Loss: 0.2230868935585022, G Loss: 5.426000118255615\n",
            "Epoch: 2, Batch: 308, D Loss: 0.22292929887771606, G Loss: 5.429353713989258\n",
            "Epoch: 2, Batch: 309, D Loss: 0.22277972102165222, G Loss: 5.432755947113037\n",
            "Epoch: 2, Batch: 310, D Loss: 0.2226254791021347, G Loss: 5.43609094619751\n",
            "Epoch: 2, Batch: 311, D Loss: 0.22246205806732178, G Loss: 5.439274311065674\n",
            "Epoch: 2, Batch: 312, D Loss: 0.22230148315429688, G Loss: 5.442380905151367\n",
            "Epoch: 2, Batch: 313, D Loss: 0.22214458882808685, G Loss: 5.445587158203125\n",
            "Epoch: 2, Batch: 314, D Loss: 0.2219909429550171, G Loss: 5.448871612548828\n",
            "Epoch: 2, Batch: 315, D Loss: 0.2218313217163086, G Loss: 5.452180862426758\n",
            "Epoch: 2, Batch: 316, D Loss: 0.22168055176734924, G Loss: 5.455555438995361\n",
            "Epoch: 2, Batch: 317, D Loss: 0.22152471542358398, G Loss: 5.458815574645996\n",
            "Epoch: 2, Batch: 318, D Loss: 0.22137030959129333, G Loss: 5.461955547332764\n",
            "Epoch: 2, Batch: 319, D Loss: 0.22121992707252502, G Loss: 5.465195655822754\n",
            "Epoch: 2, Batch: 320, D Loss: 0.22106754779815674, G Loss: 5.4686198234558105\n",
            "Epoch: 2, Batch: 321, D Loss: 0.2209121584892273, G Loss: 5.472155570983887\n",
            "Epoch: 2, Batch: 322, D Loss: 0.22075514495372772, G Loss: 5.475677490234375\n",
            "Epoch: 2, Batch: 323, D Loss: 0.2206028401851654, G Loss: 5.479180812835693\n",
            "Epoch: 2, Batch: 324, D Loss: 0.22044655680656433, G Loss: 5.4826579093933105\n",
            "Epoch: 2, Batch: 325, D Loss: 0.2202989161014557, G Loss: 5.4862470626831055\n",
            "Epoch: 2, Batch: 326, D Loss: 0.22015085816383362, G Loss: 5.490014553070068\n",
            "Epoch: 2, Batch: 327, D Loss: 0.21999502182006836, G Loss: 5.493799686431885\n",
            "Epoch: 2, Batch: 328, D Loss: 0.2198396921157837, G Loss: 5.497486114501953\n",
            "Epoch: 2, Batch: 329, D Loss: 0.21968185901641846, G Loss: 5.5006890296936035\n",
            "Epoch: 2, Batch: 330, D Loss: 0.21953609585762024, G Loss: 5.5039777755737305\n",
            "Epoch: 2, Batch: 331, D Loss: 0.21938782930374146, G Loss: 5.507429599761963\n",
            "Epoch: 2, Batch: 332, D Loss: 0.21924498677253723, G Loss: 5.511053562164307\n",
            "Epoch: 2, Batch: 333, D Loss: 0.21910014748573303, G Loss: 5.514740467071533\n",
            "Epoch: 2, Batch: 334, D Loss: 0.21895742416381836, G Loss: 5.518389701843262\n",
            "Epoch: 2, Batch: 335, D Loss: 0.21881826221942902, G Loss: 5.522036552429199\n",
            "Epoch: 2, Batch: 336, D Loss: 0.21867312490940094, G Loss: 5.52556037902832\n",
            "Epoch: 2, Batch: 337, D Loss: 0.21853165328502655, G Loss: 5.529120445251465\n",
            "Epoch: 2, Batch: 338, D Loss: 0.21838009357452393, G Loss: 5.532582759857178\n",
            "Epoch: 2, Batch: 339, D Loss: 0.21823394298553467, G Loss: 5.536008834838867\n",
            "Epoch: 2, Batch: 340, D Loss: 0.21809762716293335, G Loss: 5.539607048034668\n",
            "Epoch: 2, Batch: 341, D Loss: 0.2179490327835083, G Loss: 5.543185710906982\n",
            "Epoch: 2, Batch: 342, D Loss: 0.2178075611591339, G Loss: 5.546776294708252\n",
            "Epoch: 2, Batch: 343, D Loss: 0.21766139566898346, G Loss: 5.550283908843994\n",
            "Epoch: 2, Batch: 344, D Loss: 0.2175188958644867, G Loss: 5.553776264190674\n",
            "Epoch: 2, Batch: 345, D Loss: 0.21736964583396912, G Loss: 5.5572028160095215\n",
            "Epoch: 2, Batch: 346, D Loss: 0.2172272652387619, G Loss: 5.560701847076416\n",
            "Epoch: 2, Batch: 347, D Loss: 0.21708302199840546, G Loss: 5.564261436462402\n",
            "Epoch: 2, Batch: 348, D Loss: 0.21693921089172363, G Loss: 5.567852973937988\n",
            "Epoch: 2, Batch: 349, D Loss: 0.21679869294166565, G Loss: 5.5711164474487305\n",
            "Epoch: 2, Batch: 350, D Loss: 0.21665716171264648, G Loss: 5.57431173324585\n",
            "Epoch: 2, Batch: 351, D Loss: 0.21651563048362732, G Loss: 5.577475547790527\n",
            "Epoch: 2, Batch: 352, D Loss: 0.21637633442878723, G Loss: 5.580479621887207\n",
            "Epoch: 2, Batch: 353, D Loss: 0.2162395715713501, G Loss: 5.583310127258301\n",
            "Epoch: 2, Batch: 354, D Loss: 0.21609431505203247, G Loss: 5.58601713180542\n",
            "Epoch: 2, Batch: 355, D Loss: 0.21594873070716858, G Loss: 5.588557243347168\n",
            "Epoch: 2, Batch: 356, D Loss: 0.21580755710601807, G Loss: 5.591156482696533\n",
            "Epoch: 2, Batch: 357, D Loss: 0.21566440165042877, G Loss: 5.59396505355835\n",
            "Epoch: 2, Batch: 358, D Loss: 0.2155296504497528, G Loss: 5.597127914428711\n",
            "Epoch: 2, Batch: 359, D Loss: 0.2153867781162262, G Loss: 5.600284099578857\n",
            "Epoch: 2, Batch: 360, D Loss: 0.2152399718761444, G Loss: 5.603222370147705\n",
            "Epoch: 2, Batch: 361, D Loss: 0.21509583294391632, G Loss: 5.605971813201904\n",
            "Epoch: 2, Batch: 362, D Loss: 0.21495959162712097, G Loss: 5.608885765075684\n",
            "Epoch: 2, Batch: 363, D Loss: 0.21483120322227478, G Loss: 5.6123199462890625\n",
            "Epoch: 2, Batch: 364, D Loss: 0.21469125151634216, G Loss: 5.61594295501709\n",
            "Epoch: 2, Batch: 365, D Loss: 0.21455025672912598, G Loss: 5.619350433349609\n",
            "Epoch: 2, Batch: 366, D Loss: 0.21441635489463806, G Loss: 5.6224365234375\n",
            "Epoch: 2, Batch: 367, D Loss: 0.21428020298480988, G Loss: 5.625459671020508\n",
            "Epoch: 2, Batch: 368, D Loss: 0.21414634585380554, G Loss: 5.628582000732422\n",
            "Epoch: 2, Batch: 369, D Loss: 0.21401096880435944, G Loss: 5.631800174713135\n",
            "Epoch: 2, Batch: 370, D Loss: 0.21387916803359985, G Loss: 5.635127544403076\n",
            "Epoch: 2, Batch: 371, D Loss: 0.21374152600765228, G Loss: 5.638476371765137\n",
            "Epoch: 2, Batch: 372, D Loss: 0.21360550820827484, G Loss: 5.641794204711914\n",
            "Epoch: 2, Batch: 373, D Loss: 0.21346867084503174, G Loss: 5.645056247711182\n",
            "Epoch: 2, Batch: 374, D Loss: 0.21333549916744232, G Loss: 5.6483330726623535\n",
            "Epoch: 2, Batch: 375, D Loss: 0.2132074534893036, G Loss: 5.651806354522705\n",
            "Epoch: 2, Batch: 376, D Loss: 0.21307958662509918, G Loss: 5.655421733856201\n",
            "Epoch: 2, Batch: 377, D Loss: 0.21294790506362915, G Loss: 5.658946990966797\n",
            "Epoch: 2, Batch: 378, D Loss: 0.2128080427646637, G Loss: 5.6621994972229\n",
            "Epoch: 2, Batch: 379, D Loss: 0.21266594529151917, G Loss: 5.664852142333984\n",
            "Epoch: 2, Batch: 380, D Loss: 0.21253244578838348, G Loss: 5.667513847351074\n",
            "Epoch: 2, Batch: 381, D Loss: 0.2123986780643463, G Loss: 5.670413970947266\n",
            "Epoch: 2, Batch: 382, D Loss: 0.21226468682289124, G Loss: 5.673577785491943\n",
            "Epoch: 2, Batch: 383, D Loss: 0.21213912963867188, G Loss: 5.6769232749938965\n",
            "Epoch: 2, Batch: 384, D Loss: 0.21200765669345856, G Loss: 5.68014669418335\n",
            "Epoch: 2, Batch: 385, D Loss: 0.21188108623027802, G Loss: 5.683199882507324\n",
            "Epoch: 2, Batch: 386, D Loss: 0.21175891160964966, G Loss: 5.686270236968994\n",
            "Epoch: 2, Batch: 387, D Loss: 0.21162903308868408, G Loss: 5.689455986022949\n",
            "Epoch: 2, Batch: 388, D Loss: 0.21149390935897827, G Loss: 5.692713260650635\n",
            "Epoch: 2, Batch: 389, D Loss: 0.2113702893257141, G Loss: 5.6961283683776855\n",
            "Epoch: 2, Batch: 390, D Loss: 0.21123838424682617, G Loss: 5.6994547843933105\n",
            "Epoch: 2, Batch: 391, D Loss: 0.21110282838344574, G Loss: 5.702569484710693\n",
            "Epoch: 2, Batch: 392, D Loss: 0.21097970008850098, G Loss: 5.7058234214782715\n",
            "Epoch: 2, Batch: 393, D Loss: 0.21085485816001892, G Loss: 5.709293842315674\n",
            "Epoch: 2, Batch: 394, D Loss: 0.2107183039188385, G Loss: 5.712742328643799\n",
            "Epoch: 2, Batch: 395, D Loss: 0.2105957269668579, G Loss: 5.7161946296691895\n",
            "Epoch: 2, Batch: 396, D Loss: 0.2104627788066864, G Loss: 5.719526767730713\n",
            "Epoch: 2, Batch: 397, D Loss: 0.21033775806427002, G Loss: 5.722904682159424\n",
            "Epoch: 2, Batch: 398, D Loss: 0.21020545065402985, G Loss: 5.726316452026367\n",
            "Epoch: 2, Batch: 399, D Loss: 0.2100794017314911, G Loss: 5.729776382446289\n",
            "Epoch: 2, Batch: 400, D Loss: 0.2099463790655136, G Loss: 5.733160018920898\n",
            "Epoch: 2, Batch: 401, D Loss: 0.20982101559638977, G Loss: 5.736289978027344\n",
            "Epoch: 2, Batch: 402, D Loss: 0.20969876646995544, G Loss: 5.7394328117370605\n",
            "Epoch: 2, Batch: 403, D Loss: 0.20956943929195404, G Loss: 5.74254035949707\n",
            "Epoch: 2, Batch: 404, D Loss: 0.20943894982337952, G Loss: 5.745548248291016\n",
            "Epoch: 2, Batch: 405, D Loss: 0.2093125581741333, G Loss: 5.748552322387695\n",
            "Epoch: 2, Batch: 406, D Loss: 0.20919311046600342, G Loss: 5.7517290115356445\n",
            "Epoch: 2, Batch: 407, D Loss: 0.2090650051832199, G Loss: 5.754769802093506\n",
            "Epoch: 2, Batch: 408, D Loss: 0.2089376151561737, G Loss: 5.75766658782959\n",
            "Epoch: 2, Batch: 409, D Loss: 0.20881494879722595, G Loss: 5.760498523712158\n",
            "Epoch: 2, Batch: 410, D Loss: 0.20868980884552002, G Loss: 5.763321399688721\n",
            "Epoch: 2, Batch: 411, D Loss: 0.20856812596321106, G Loss: 5.7661871910095215\n",
            "Epoch: 2, Batch: 412, D Loss: 0.20844678580760956, G Loss: 5.7691144943237305\n",
            "Epoch: 2, Batch: 413, D Loss: 0.20832303166389465, G Loss: 5.771947383880615\n",
            "Epoch: 2, Batch: 414, D Loss: 0.20819421112537384, G Loss: 5.774571895599365\n",
            "Epoch: 2, Batch: 415, D Loss: 0.20806464552879333, G Loss: 5.7771315574646\n",
            "Epoch: 2, Batch: 416, D Loss: 0.20795010030269623, G Loss: 5.780043601989746\n",
            "Epoch: 2, Batch: 417, D Loss: 0.20783041417598724, G Loss: 5.78334379196167\n",
            "Epoch: 2, Batch: 418, D Loss: 0.20770619809627533, G Loss: 5.786484241485596\n",
            "Epoch: 2, Batch: 419, D Loss: 0.2075832039117813, G Loss: 5.789414405822754\n",
            "Epoch: 2, Batch: 420, D Loss: 0.20745567977428436, G Loss: 5.792212009429932\n",
            "Epoch: 2, Batch: 421, D Loss: 0.20733647048473358, G Loss: 5.795114040374756\n",
            "Epoch: 2, Batch: 422, D Loss: 0.20721065998077393, G Loss: 5.798245429992676\n",
            "Epoch: 2, Batch: 423, D Loss: 0.20709332823753357, G Loss: 5.801620960235596\n",
            "Epoch: 2, Batch: 424, D Loss: 0.206975057721138, G Loss: 5.805011749267578\n",
            "Epoch: 2, Batch: 425, D Loss: 0.20685788989067078, G Loss: 5.808237552642822\n",
            "Epoch: 2, Batch: 426, D Loss: 0.20674119889736176, G Loss: 5.811326503753662\n",
            "Epoch: 2, Batch: 427, D Loss: 0.2066274881362915, G Loss: 5.814511775970459\n",
            "Epoch: 2, Batch: 428, D Loss: 0.20651289820671082, G Loss: 5.817814350128174\n",
            "Epoch: 2, Batch: 429, D Loss: 0.20639079809188843, G Loss: 5.820978164672852\n",
            "Epoch: 2, Batch: 430, D Loss: 0.20627418160438538, G Loss: 5.824119567871094\n",
            "Epoch: 2, Batch: 431, D Loss: 0.20615386962890625, G Loss: 5.827292442321777\n",
            "Epoch: 2, Batch: 432, D Loss: 0.20603540539741516, G Loss: 5.830509662628174\n",
            "Epoch: 2, Batch: 433, D Loss: 0.20592400431632996, G Loss: 5.833853721618652\n",
            "Epoch: 2, Batch: 434, D Loss: 0.20580890774726868, G Loss: 5.837217807769775\n",
            "Epoch: 2, Batch: 435, D Loss: 0.20569190382957458, G Loss: 5.8404741287231445\n",
            "Epoch: 2, Batch: 436, D Loss: 0.2055816352367401, G Loss: 5.843747615814209\n",
            "Epoch: 2, Batch: 437, D Loss: 0.20546557009220123, G Loss: 5.847036361694336\n",
            "Epoch: 2, Batch: 438, D Loss: 0.2053583264350891, G Loss: 5.850405693054199\n",
            "Epoch: 2, Batch: 439, D Loss: 0.20524051785469055, G Loss: 5.8537187576293945\n",
            "Epoch: 2, Batch: 440, D Loss: 0.20512306690216064, G Loss: 5.856911659240723\n",
            "Epoch: 2, Batch: 441, D Loss: 0.2050054669380188, G Loss: 5.860069274902344\n",
            "Epoch: 2, Batch: 442, D Loss: 0.20488357543945312, G Loss: 5.8632307052612305\n",
            "Epoch: 2, Batch: 443, D Loss: 0.20476824045181274, G Loss: 5.866546154022217\n",
            "Epoch: 2, Batch: 444, D Loss: 0.20465433597564697, G Loss: 5.869974613189697\n",
            "Epoch: 2, Batch: 445, D Loss: 0.2045499086380005, G Loss: 5.873531341552734\n",
            "Epoch: 2, Batch: 446, D Loss: 0.20443524420261383, G Loss: 5.876844882965088\n",
            "Epoch: 2, Batch: 447, D Loss: 0.204321950674057, G Loss: 5.879913806915283\n",
            "Epoch: 2, Batch: 448, D Loss: 0.20421501994132996, G Loss: 5.883048057556152\n",
            "Epoch: 2, Batch: 449, D Loss: 0.2041008174419403, G Loss: 5.886330604553223\n",
            "Epoch: 2, Batch: 450, D Loss: 0.2039843499660492, G Loss: 5.889706134796143\n",
            "Epoch: 2, Batch: 451, D Loss: 0.2038746178150177, G Loss: 5.893179416656494\n",
            "Epoch: 2, Batch: 452, D Loss: 0.2037629783153534, G Loss: 5.896603107452393\n",
            "Epoch: 2, Batch: 453, D Loss: 0.2036501169204712, G Loss: 5.899881839752197\n",
            "Epoch: 2, Batch: 454, D Loss: 0.20353534817695618, G Loss: 5.90308141708374\n",
            "Epoch: 2, Batch: 455, D Loss: 0.20342203974723816, G Loss: 5.906265735626221\n",
            "Epoch: 2, Batch: 456, D Loss: 0.2033093273639679, G Loss: 5.909616947174072\n",
            "Epoch: 2, Batch: 457, D Loss: 0.2031954973936081, G Loss: 5.913076877593994\n",
            "Epoch: 2, Batch: 458, D Loss: 0.2030796855688095, G Loss: 5.916515350341797\n",
            "Epoch: 2, Batch: 459, D Loss: 0.2029675841331482, G Loss: 5.919877529144287\n",
            "Epoch: 2, Batch: 460, D Loss: 0.20285476744174957, G Loss: 5.923192024230957\n",
            "Epoch: 2, Batch: 461, D Loss: 0.2027447521686554, G Loss: 5.926541805267334\n",
            "Epoch: 2, Batch: 462, D Loss: 0.20263689756393433, G Loss: 5.929986953735352\n",
            "Epoch: 2, Batch: 463, D Loss: 0.20252233743667603, G Loss: 5.9333815574646\n",
            "Epoch: 2, Batch: 464, D Loss: 0.2024100422859192, G Loss: 5.9367570877075195\n",
            "Epoch: 2, Batch: 465, D Loss: 0.2022990882396698, G Loss: 5.940118312835693\n",
            "Epoch: 2, Batch: 466, D Loss: 0.2021854817867279, G Loss: 5.94342041015625\n",
            "Epoch: 2, Batch: 467, D Loss: 0.20207656919956207, G Loss: 5.9467878341674805\n",
            "Epoch: 3, Batch: 0, D Loss: 0.20196889340877533, G Loss: 5.950228214263916\n",
            "Epoch: 3, Batch: 1, D Loss: 0.2018536925315857, G Loss: 5.953505992889404\n",
            "Epoch: 3, Batch: 2, D Loss: 0.20174376666545868, G Loss: 5.9566650390625\n",
            "Epoch: 3, Batch: 3, D Loss: 0.20163530111312866, G Loss: 5.959803581237793\n",
            "Epoch: 3, Batch: 4, D Loss: 0.20153510570526123, G Loss: 5.963092803955078\n",
            "Epoch: 3, Batch: 5, D Loss: 0.20142877101898193, G Loss: 5.966437816619873\n",
            "Epoch: 3, Batch: 6, D Loss: 0.20131909847259521, G Loss: 5.969661235809326\n",
            "Epoch: 3, Batch: 7, D Loss: 0.20120859146118164, G Loss: 5.97276496887207\n",
            "Epoch: 3, Batch: 8, D Loss: 0.20109623670578003, G Loss: 5.975813865661621\n",
            "Epoch: 3, Batch: 9, D Loss: 0.20098748803138733, G Loss: 5.978963851928711\n",
            "Epoch: 3, Batch: 10, D Loss: 0.200876384973526, G Loss: 5.982240676879883\n",
            "Epoch: 3, Batch: 11, D Loss: 0.20077118277549744, G Loss: 5.985693454742432\n",
            "Epoch: 3, Batch: 12, D Loss: 0.200668603181839, G Loss: 5.989175319671631\n",
            "Epoch: 3, Batch: 13, D Loss: 0.2005552053451538, G Loss: 5.992396354675293\n",
            "Epoch: 3, Batch: 14, D Loss: 0.20044732093811035, G Loss: 5.9954833984375\n",
            "Epoch: 3, Batch: 15, D Loss: 0.2003413736820221, G Loss: 5.998687267303467\n",
            "Epoch: 3, Batch: 16, D Loss: 0.20022834837436676, G Loss: 6.002037525177002\n",
            "Epoch: 3, Batch: 17, D Loss: 0.20012232661247253, G Loss: 6.005493640899658\n",
            "Epoch: 3, Batch: 18, D Loss: 0.20002295076847076, G Loss: 6.0090179443359375\n",
            "Epoch: 3, Batch: 19, D Loss: 0.19992101192474365, G Loss: 6.012364864349365\n",
            "Epoch: 3, Batch: 20, D Loss: 0.1998170018196106, G Loss: 6.0155510902404785\n",
            "Epoch: 3, Batch: 21, D Loss: 0.1997145116329193, G Loss: 6.018745422363281\n",
            "Epoch: 3, Batch: 22, D Loss: 0.19961054623126984, G Loss: 6.0220627784729\n",
            "Epoch: 3, Batch: 23, D Loss: 0.19950902462005615, G Loss: 6.025537014007568\n",
            "Epoch: 3, Batch: 24, D Loss: 0.19941109418869019, G Loss: 6.028987884521484\n",
            "Epoch: 3, Batch: 25, D Loss: 0.1993112862110138, G Loss: 6.032336711883545\n",
            "Epoch: 3, Batch: 26, D Loss: 0.19920937716960907, G Loss: 6.035548686981201\n",
            "Epoch: 3, Batch: 27, D Loss: 0.19911083579063416, G Loss: 6.038810729980469\n",
            "Epoch: 3, Batch: 28, D Loss: 0.19901010394096375, G Loss: 6.042162895202637\n",
            "Epoch: 3, Batch: 29, D Loss: 0.19890712201595306, G Loss: 6.045454025268555\n",
            "Epoch: 3, Batch: 30, D Loss: 0.1987985372543335, G Loss: 6.048593997955322\n",
            "Epoch: 3, Batch: 31, D Loss: 0.1986950933933258, G Loss: 6.051687717437744\n",
            "Epoch: 3, Batch: 32, D Loss: 0.19858771562576294, G Loss: 6.054683685302734\n",
            "Epoch: 3, Batch: 33, D Loss: 0.19848480820655823, G Loss: 6.057775974273682\n",
            "Epoch: 3, Batch: 34, D Loss: 0.19837993383407593, G Loss: 6.060932159423828\n",
            "Epoch: 3, Batch: 35, D Loss: 0.19828161597251892, G Loss: 6.064151287078857\n",
            "Epoch: 3, Batch: 36, D Loss: 0.1981835961341858, G Loss: 6.067380428314209\n",
            "Epoch: 3, Batch: 37, D Loss: 0.19807729125022888, G Loss: 6.070418357849121\n",
            "Epoch: 3, Batch: 38, D Loss: 0.1979803442955017, G Loss: 6.073458671569824\n",
            "Epoch: 3, Batch: 39, D Loss: 0.19787953794002533, G Loss: 6.076582908630371\n",
            "Epoch: 3, Batch: 40, D Loss: 0.19778604805469513, G Loss: 6.079892158508301\n",
            "Epoch: 3, Batch: 41, D Loss: 0.1976853311061859, G Loss: 6.083159446716309\n",
            "Epoch: 3, Batch: 42, D Loss: 0.19758731126785278, G Loss: 6.086356163024902\n",
            "Epoch: 3, Batch: 43, D Loss: 0.19748423993587494, G Loss: 6.089498996734619\n",
            "Epoch: 3, Batch: 44, D Loss: 0.19738590717315674, G Loss: 6.09268856048584\n",
            "Epoch: 3, Batch: 45, D Loss: 0.19728845357894897, G Loss: 6.095992088317871\n",
            "Epoch: 3, Batch: 46, D Loss: 0.19718733429908752, G Loss: 6.099277019500732\n",
            "Epoch: 3, Batch: 47, D Loss: 0.1970866620540619, G Loss: 6.102570056915283\n",
            "Epoch: 3, Batch: 48, D Loss: 0.19697925448417664, G Loss: 6.105764865875244\n",
            "Epoch: 3, Batch: 49, D Loss: 0.19688014686107635, G Loss: 6.108999252319336\n",
            "Epoch: 3, Batch: 50, D Loss: 0.19678016006946564, G Loss: 6.11231803894043\n",
            "Epoch: 3, Batch: 51, D Loss: 0.19667492806911469, G Loss: 6.11561918258667\n",
            "Epoch: 3, Batch: 52, D Loss: 0.19657960534095764, G Loss: 6.118913650512695\n",
            "Epoch: 3, Batch: 53, D Loss: 0.19647301733493805, G Loss: 6.122125148773193\n",
            "Epoch: 3, Batch: 54, D Loss: 0.19637686014175415, G Loss: 6.125396728515625\n",
            "Epoch: 3, Batch: 55, D Loss: 0.19627468287944794, G Loss: 6.128677845001221\n",
            "Epoch: 3, Batch: 56, D Loss: 0.19617587327957153, G Loss: 6.131855010986328\n",
            "Epoch: 3, Batch: 57, D Loss: 0.196076437830925, G Loss: 6.134968280792236\n",
            "Epoch: 3, Batch: 58, D Loss: 0.19598515331745148, G Loss: 6.1382155418396\n",
            "Epoch: 3, Batch: 59, D Loss: 0.19588235020637512, G Loss: 6.141425132751465\n",
            "Epoch: 3, Batch: 60, D Loss: 0.19578516483306885, G Loss: 6.144529342651367\n",
            "Epoch: 3, Batch: 61, D Loss: 0.1956893801689148, G Loss: 6.147609710693359\n",
            "Epoch: 3, Batch: 62, D Loss: 0.1955912560224533, G Loss: 6.150674819946289\n",
            "Epoch: 3, Batch: 63, D Loss: 0.1954960823059082, G Loss: 6.153796672821045\n",
            "Epoch: 3, Batch: 64, D Loss: 0.1954064965248108, G Loss: 6.157038688659668\n",
            "Epoch: 3, Batch: 65, D Loss: 0.19531571865081787, G Loss: 6.160271167755127\n",
            "Epoch: 3, Batch: 66, D Loss: 0.19521775841712952, G Loss: 6.163298606872559\n",
            "Epoch: 3, Batch: 67, D Loss: 0.1951269507408142, G Loss: 6.166235446929932\n",
            "Epoch: 3, Batch: 68, D Loss: 0.1950322985649109, G Loss: 6.1692304611206055\n",
            "Epoch: 3, Batch: 69, D Loss: 0.1949368417263031, G Loss: 6.172330856323242\n",
            "Epoch: 3, Batch: 70, D Loss: 0.19484573602676392, G Loss: 6.175555229187012\n",
            "Epoch: 3, Batch: 71, D Loss: 0.19475209712982178, G Loss: 6.178679466247559\n",
            "Epoch: 3, Batch: 72, D Loss: 0.1946498453617096, G Loss: 6.1815385818481445\n",
            "Epoch: 3, Batch: 73, D Loss: 0.1945537030696869, G Loss: 6.184284687042236\n",
            "Epoch: 3, Batch: 74, D Loss: 0.19445553421974182, G Loss: 6.1871562004089355\n",
            "Epoch: 3, Batch: 75, D Loss: 0.1943616271018982, G Loss: 6.190267562866211\n",
            "Epoch: 3, Batch: 76, D Loss: 0.19427277147769928, G Loss: 6.19350004196167\n",
            "Epoch: 3, Batch: 77, D Loss: 0.19417919218540192, G Loss: 6.19657039642334\n",
            "Epoch: 3, Batch: 78, D Loss: 0.19409127533435822, G Loss: 6.199565887451172\n",
            "Epoch: 3, Batch: 79, D Loss: 0.19400319457054138, G Loss: 6.202587127685547\n",
            "Epoch: 3, Batch: 80, D Loss: 0.1939052939414978, G Loss: 6.2055559158325195\n",
            "Epoch: 3, Batch: 81, D Loss: 0.19381728768348694, G Loss: 6.208592891693115\n",
            "Epoch: 3, Batch: 82, D Loss: 0.1937296837568283, G Loss: 6.211766242980957\n",
            "Epoch: 3, Batch: 83, D Loss: 0.19363786280155182, G Loss: 6.214916229248047\n",
            "Epoch: 3, Batch: 84, D Loss: 0.19355036318302155, G Loss: 6.218035697937012\n",
            "Epoch: 3, Batch: 85, D Loss: 0.19345557689666748, G Loss: 6.220980167388916\n",
            "Epoch: 3, Batch: 86, D Loss: 0.19336336851119995, G Loss: 6.2238664627075195\n",
            "Epoch: 3, Batch: 87, D Loss: 0.1932770162820816, G Loss: 6.22694206237793\n",
            "Epoch: 3, Batch: 88, D Loss: 0.19318506121635437, G Loss: 6.229929447174072\n",
            "Epoch: 3, Batch: 89, D Loss: 0.19308635592460632, G Loss: 6.232656955718994\n",
            "Epoch: 3, Batch: 90, D Loss: 0.19300034642219543, G Loss: 6.23537540435791\n",
            "Epoch: 3, Batch: 91, D Loss: 0.1929149478673935, G Loss: 6.238277912139893\n",
            "Epoch: 3, Batch: 92, D Loss: 0.19282101094722748, G Loss: 6.241183280944824\n",
            "Epoch: 3, Batch: 93, D Loss: 0.19272753596305847, G Loss: 6.24393892288208\n",
            "Epoch: 3, Batch: 94, D Loss: 0.19263407588005066, G Loss: 6.246641159057617\n",
            "Epoch: 3, Batch: 95, D Loss: 0.19254890084266663, G Loss: 6.2494940757751465\n",
            "Epoch: 3, Batch: 96, D Loss: 0.19246605038642883, G Loss: 6.252597332000732\n",
            "Epoch: 3, Batch: 97, D Loss: 0.192381352186203, G Loss: 6.255743503570557\n",
            "Epoch: 3, Batch: 98, D Loss: 0.19229453802108765, G Loss: 6.258678913116455\n",
            "Epoch: 3, Batch: 99, D Loss: 0.1922065168619156, G Loss: 6.2613959312438965\n",
            "Epoch: 3, Batch: 100, D Loss: 0.19211222231388092, G Loss: 6.264003276824951\n",
            "Epoch: 3, Batch: 101, D Loss: 0.19201871752738953, G Loss: 6.266674518585205\n",
            "Epoch: 3, Batch: 102, D Loss: 0.1919344961643219, G Loss: 6.269655227661133\n",
            "Epoch: 3, Batch: 103, D Loss: 0.19184847176074982, G Loss: 6.272807598114014\n",
            "Epoch: 3, Batch: 104, D Loss: 0.19176125526428223, G Loss: 6.275818347930908\n",
            "Epoch: 3, Batch: 105, D Loss: 0.19167539477348328, G Loss: 6.278689384460449\n",
            "Epoch: 3, Batch: 106, D Loss: 0.19158731400966644, G Loss: 6.281500816345215\n",
            "Epoch: 3, Batch: 107, D Loss: 0.19149568676948547, G Loss: 6.284351825714111\n",
            "Epoch: 3, Batch: 108, D Loss: 0.19140970706939697, G Loss: 6.287387847900391\n",
            "Epoch: 3, Batch: 109, D Loss: 0.1913192719221115, G Loss: 6.290428161621094\n",
            "Epoch: 3, Batch: 110, D Loss: 0.1912277340888977, G Loss: 6.293348789215088\n",
            "Epoch: 3, Batch: 111, D Loss: 0.19113817811012268, G Loss: 6.296173095703125\n",
            "Epoch: 3, Batch: 112, D Loss: 0.19105134904384613, G Loss: 6.299072742462158\n",
            "Epoch: 3, Batch: 113, D Loss: 0.19096243381500244, G Loss: 6.302104473114014\n",
            "Epoch: 3, Batch: 114, D Loss: 0.19087356328964233, G Loss: 6.305118083953857\n",
            "Epoch: 3, Batch: 115, D Loss: 0.1907919943332672, G Loss: 6.30815315246582\n",
            "Epoch: 3, Batch: 116, D Loss: 0.19070783257484436, G Loss: 6.311212062835693\n",
            "Epoch: 3, Batch: 117, D Loss: 0.19062156975269318, G Loss: 6.314216613769531\n",
            "Epoch: 3, Batch: 118, D Loss: 0.19053198397159576, G Loss: 6.317137241363525\n",
            "Epoch: 3, Batch: 119, D Loss: 0.19044454395771027, G Loss: 6.320065498352051\n",
            "Epoch: 3, Batch: 120, D Loss: 0.1903637796640396, G Loss: 6.323148250579834\n",
            "Epoch: 3, Batch: 121, D Loss: 0.1902788281440735, G Loss: 6.326266765594482\n",
            "Epoch: 3, Batch: 122, D Loss: 0.1901954859495163, G Loss: 6.329285621643066\n",
            "Epoch: 3, Batch: 123, D Loss: 0.19010592997074127, G Loss: 6.332081317901611\n",
            "Epoch: 3, Batch: 124, D Loss: 0.19002345204353333, G Loss: 6.3349080085754395\n",
            "Epoch: 3, Batch: 125, D Loss: 0.18994015455245972, G Loss: 6.337896347045898\n",
            "Epoch: 3, Batch: 126, D Loss: 0.189858078956604, G Loss: 6.341010570526123\n",
            "Epoch: 3, Batch: 127, D Loss: 0.1897740364074707, G Loss: 6.344043731689453\n",
            "Epoch: 3, Batch: 128, D Loss: 0.18968984484672546, G Loss: 6.3468918800354\n",
            "Epoch: 3, Batch: 129, D Loss: 0.1896088421344757, G Loss: 6.349747657775879\n",
            "Epoch: 3, Batch: 130, D Loss: 0.18953031301498413, G Loss: 6.352797985076904\n",
            "Epoch: 3, Batch: 131, D Loss: 0.18944424390792847, G Loss: 6.355815410614014\n",
            "Epoch: 3, Batch: 132, D Loss: 0.18935737013816833, G Loss: 6.358659744262695\n",
            "Epoch: 3, Batch: 133, D Loss: 0.1892700344324112, G Loss: 6.36137580871582\n",
            "Epoch: 3, Batch: 134, D Loss: 0.18918749690055847, G Loss: 6.364154815673828\n",
            "Epoch: 3, Batch: 135, D Loss: 0.18910588324069977, G Loss: 6.367149353027344\n",
            "Epoch: 3, Batch: 136, D Loss: 0.18901775777339935, G Loss: 6.3701372146606445\n",
            "Epoch: 3, Batch: 137, D Loss: 0.18893437087535858, G Loss: 6.373001575469971\n",
            "Epoch: 3, Batch: 138, D Loss: 0.18884965777397156, G Loss: 6.375803470611572\n",
            "Epoch: 3, Batch: 139, D Loss: 0.18877002596855164, G Loss: 6.378726005554199\n",
            "Epoch: 3, Batch: 140, D Loss: 0.18868644535541534, G Loss: 6.381709575653076\n",
            "Epoch: 3, Batch: 141, D Loss: 0.18860507011413574, G Loss: 6.3846869468688965\n",
            "Epoch: 3, Batch: 142, D Loss: 0.18852940201759338, G Loss: 6.387711524963379\n",
            "Epoch: 3, Batch: 143, D Loss: 0.18844930827617645, G Loss: 6.390688419342041\n",
            "Epoch: 3, Batch: 144, D Loss: 0.1883624792098999, G Loss: 6.393448352813721\n",
            "Epoch: 3, Batch: 145, D Loss: 0.18828085064888, G Loss: 6.3961381912231445\n",
            "Epoch: 3, Batch: 146, D Loss: 0.18820470571517944, G Loss: 6.399061679840088\n",
            "Epoch: 3, Batch: 147, D Loss: 0.18811887502670288, G Loss: 6.402111530303955\n",
            "Epoch: 3, Batch: 148, D Loss: 0.1880386769771576, G Loss: 6.40513277053833\n",
            "Epoch: 3, Batch: 149, D Loss: 0.1879521608352661, G Loss: 6.407909870147705\n",
            "Epoch: 3, Batch: 150, D Loss: 0.1878681182861328, G Loss: 6.410590648651123\n",
            "Epoch: 3, Batch: 151, D Loss: 0.18778738379478455, G Loss: 6.413422584533691\n",
            "Epoch: 3, Batch: 152, D Loss: 0.18770664930343628, G Loss: 6.416464805603027\n",
            "Epoch: 3, Batch: 153, D Loss: 0.18762996792793274, G Loss: 6.419615268707275\n",
            "Epoch: 3, Batch: 154, D Loss: 0.18755662441253662, G Loss: 6.422745227813721\n",
            "Epoch: 3, Batch: 155, D Loss: 0.18747049570083618, G Loss: 6.425585746765137\n",
            "Epoch: 3, Batch: 156, D Loss: 0.1873926818370819, G Loss: 6.42830753326416\n",
            "Epoch: 3, Batch: 157, D Loss: 0.1873094141483307, G Loss: 6.431112766265869\n",
            "Epoch: 3, Batch: 158, D Loss: 0.1872333288192749, G Loss: 6.434195518493652\n",
            "Epoch: 3, Batch: 159, D Loss: 0.18715181946754456, G Loss: 6.437340259552002\n",
            "Epoch: 3, Batch: 160, D Loss: 0.1870727837085724, G Loss: 6.4403791427612305\n",
            "Epoch: 3, Batch: 161, D Loss: 0.1869959533214569, G Loss: 6.443337440490723\n",
            "Epoch: 3, Batch: 162, D Loss: 0.18691205978393555, G Loss: 6.446189880371094\n",
            "Epoch: 3, Batch: 163, D Loss: 0.18683438003063202, G Loss: 6.449130058288574\n",
            "Epoch: 3, Batch: 164, D Loss: 0.1867521107196808, G Loss: 6.452145099639893\n",
            "Epoch: 3, Batch: 165, D Loss: 0.18667596578598022, G Loss: 6.455260276794434\n",
            "Epoch: 3, Batch: 166, D Loss: 0.18659578263759613, G Loss: 6.458302974700928\n",
            "Epoch: 3, Batch: 167, D Loss: 0.18651705980300903, G Loss: 6.461240291595459\n",
            "Epoch: 3, Batch: 168, D Loss: 0.18643906712532043, G Loss: 6.464171409606934\n",
            "Epoch: 3, Batch: 169, D Loss: 0.18635985255241394, G Loss: 6.46719217300415\n",
            "Epoch: 3, Batch: 170, D Loss: 0.1862799972295761, G Loss: 6.470198154449463\n",
            "Epoch: 3, Batch: 171, D Loss: 0.18620233237743378, G Loss: 6.47315788269043\n",
            "Epoch: 3, Batch: 172, D Loss: 0.18612569570541382, G Loss: 6.476100444793701\n",
            "Epoch: 3, Batch: 173, D Loss: 0.1860453188419342, G Loss: 6.479021072387695\n",
            "Epoch: 3, Batch: 174, D Loss: 0.18596889078617096, G Loss: 6.481975078582764\n",
            "Epoch: 3, Batch: 175, D Loss: 0.18588536977767944, G Loss: 6.484821796417236\n",
            "Epoch: 3, Batch: 176, D Loss: 0.18581122159957886, G Loss: 6.487717151641846\n",
            "Epoch: 3, Batch: 177, D Loss: 0.18573695421218872, G Loss: 6.490720272064209\n",
            "Epoch: 3, Batch: 178, D Loss: 0.18565817177295685, G Loss: 6.493683815002441\n",
            "Epoch: 3, Batch: 179, D Loss: 0.1855805516242981, G Loss: 6.496572494506836\n",
            "Epoch: 3, Batch: 180, D Loss: 0.18550775945186615, G Loss: 6.499514102935791\n",
            "Epoch: 3, Batch: 181, D Loss: 0.18542896211147308, G Loss: 6.502460956573486\n",
            "Epoch: 3, Batch: 182, D Loss: 0.18535223603248596, G Loss: 6.505410194396973\n",
            "Epoch: 3, Batch: 183, D Loss: 0.18527628481388092, G Loss: 6.508349895477295\n",
            "Epoch: 3, Batch: 184, D Loss: 0.18519917130470276, G Loss: 6.511199474334717\n",
            "Epoch: 3, Batch: 185, D Loss: 0.18512576818466187, G Loss: 6.514034271240234\n",
            "Epoch: 3, Batch: 186, D Loss: 0.1850496232509613, G Loss: 6.516897201538086\n",
            "Epoch: 3, Batch: 187, D Loss: 0.18497060239315033, G Loss: 6.519710540771484\n",
            "Epoch: 3, Batch: 188, D Loss: 0.18489450216293335, G Loss: 6.522497653961182\n",
            "Epoch: 3, Batch: 189, D Loss: 0.18482321500778198, G Loss: 6.5253825187683105\n",
            "Epoch: 3, Batch: 190, D Loss: 0.18475106358528137, G Loss: 6.528296947479248\n",
            "Epoch: 3, Batch: 191, D Loss: 0.1846769154071808, G Loss: 6.531147480010986\n",
            "Epoch: 3, Batch: 192, D Loss: 0.18460384011268616, G Loss: 6.533910751342773\n",
            "Epoch: 3, Batch: 193, D Loss: 0.18452349305152893, G Loss: 6.536553382873535\n",
            "Epoch: 3, Batch: 194, D Loss: 0.18445110321044922, G Loss: 6.539305686950684\n",
            "Epoch: 3, Batch: 195, D Loss: 0.18438079953193665, G Loss: 6.542300701141357\n",
            "Epoch: 3, Batch: 196, D Loss: 0.18431103229522705, G Loss: 6.545321941375732\n",
            "Epoch: 3, Batch: 197, D Loss: 0.1842396855354309, G Loss: 6.548173904418945\n",
            "Epoch: 3, Batch: 198, D Loss: 0.1841617226600647, G Loss: 6.550716400146484\n",
            "Epoch: 3, Batch: 199, D Loss: 0.18408624827861786, G Loss: 6.553208351135254\n",
            "Epoch: 3, Batch: 200, D Loss: 0.18401378393173218, G Loss: 6.555978775024414\n",
            "Epoch: 3, Batch: 201, D Loss: 0.18394234776496887, G Loss: 6.558998107910156\n",
            "Epoch: 3, Batch: 202, D Loss: 0.18387168645858765, G Loss: 6.561861038208008\n",
            "Epoch: 3, Batch: 203, D Loss: 0.1838039606809616, G Loss: 6.56460428237915\n",
            "Epoch: 3, Batch: 204, D Loss: 0.1837323009967804, G Loss: 6.567282199859619\n",
            "Epoch: 3, Batch: 205, D Loss: 0.18365338444709778, G Loss: 6.569895267486572\n",
            "Epoch: 3, Batch: 206, D Loss: 0.18357715010643005, G Loss: 6.572511196136475\n",
            "Epoch: 3, Batch: 207, D Loss: 0.1834985315799713, G Loss: 6.575155258178711\n",
            "Epoch: 3, Batch: 208, D Loss: 0.18342779576778412, G Loss: 6.577942848205566\n",
            "Epoch: 3, Batch: 209, D Loss: 0.183359295129776, G Loss: 6.580837249755859\n",
            "Epoch: 3, Batch: 210, D Loss: 0.18328925967216492, G Loss: 6.583644866943359\n",
            "Epoch: 3, Batch: 211, D Loss: 0.1832141876220703, G Loss: 6.586226940155029\n",
            "Epoch: 3, Batch: 212, D Loss: 0.18313883244991302, G Loss: 6.588712215423584\n",
            "Epoch: 3, Batch: 213, D Loss: 0.1830621361732483, G Loss: 6.591279983520508\n",
            "Epoch: 3, Batch: 214, D Loss: 0.1829874962568283, G Loss: 6.594020366668701\n",
            "Epoch: 3, Batch: 215, D Loss: 0.18291640281677246, G Loss: 6.596917152404785\n",
            "Epoch: 3, Batch: 216, D Loss: 0.18284210562705994, G Loss: 6.599684238433838\n",
            "Epoch: 3, Batch: 217, D Loss: 0.1827668845653534, G Loss: 6.602254867553711\n",
            "Epoch: 3, Batch: 218, D Loss: 0.18270540237426758, G Loss: 6.605000019073486\n",
            "Epoch: 3, Batch: 219, D Loss: 0.18263788521289825, G Loss: 6.607954025268555\n",
            "Epoch: 3, Batch: 220, D Loss: 0.18256407976150513, G Loss: 6.6108622550964355\n",
            "Epoch: 3, Batch: 221, D Loss: 0.18249571323394775, G Loss: 6.613607406616211\n",
            "Epoch: 3, Batch: 222, D Loss: 0.18242350220680237, G Loss: 6.616251468658447\n",
            "Epoch: 3, Batch: 223, D Loss: 0.18235212564468384, G Loss: 6.618958950042725\n",
            "Epoch: 3, Batch: 224, D Loss: 0.1822807490825653, G Loss: 6.6218156814575195\n",
            "Epoch: 3, Batch: 225, D Loss: 0.18221482634544373, G Loss: 6.624805450439453\n",
            "Epoch: 3, Batch: 226, D Loss: 0.18213969469070435, G Loss: 6.627673625946045\n",
            "Epoch: 3, Batch: 227, D Loss: 0.18206819891929626, G Loss: 6.630397796630859\n",
            "Epoch: 3, Batch: 228, D Loss: 0.18199965357780457, G Loss: 6.633145332336426\n",
            "Epoch: 3, Batch: 229, D Loss: 0.18192952871322632, G Loss: 6.636014938354492\n",
            "Epoch: 3, Batch: 230, D Loss: 0.18186403810977936, G Loss: 6.638988971710205\n",
            "Epoch: 3, Batch: 231, D Loss: 0.18179263174533844, G Loss: 6.641841888427734\n",
            "Epoch: 3, Batch: 232, D Loss: 0.18172399699687958, G Loss: 6.644585609436035\n",
            "Epoch: 3, Batch: 233, D Loss: 0.18165189027786255, G Loss: 6.647262096405029\n",
            "Epoch: 3, Batch: 234, D Loss: 0.1815839409828186, G Loss: 6.650073528289795\n",
            "Epoch: 3, Batch: 235, D Loss: 0.1815098226070404, G Loss: 6.652909278869629\n",
            "Epoch: 3, Batch: 236, D Loss: 0.18144363164901733, G Loss: 6.655789375305176\n",
            "Epoch: 3, Batch: 237, D Loss: 0.18137791752815247, G Loss: 6.658663749694824\n",
            "Epoch: 3, Batch: 238, D Loss: 0.18130898475646973, G Loss: 6.661448001861572\n",
            "Epoch: 3, Batch: 239, D Loss: 0.18123598396778107, G Loss: 6.6641011238098145\n",
            "Epoch: 3, Batch: 240, D Loss: 0.18116414546966553, G Loss: 6.6667585372924805\n",
            "Epoch: 3, Batch: 241, D Loss: 0.18109434843063354, G Loss: 6.669557571411133\n",
            "Epoch: 3, Batch: 242, D Loss: 0.1810281127691269, G Loss: 6.672475337982178\n",
            "Epoch: 3, Batch: 243, D Loss: 0.180953711271286, G Loss: 6.675232887268066\n",
            "Epoch: 3, Batch: 244, D Loss: 0.18088701367378235, G Loss: 6.6779680252075195\n",
            "Epoch: 3, Batch: 245, D Loss: 0.18081416189670563, G Loss: 6.680722236633301\n",
            "Epoch: 3, Batch: 246, D Loss: 0.18074777722358704, G Loss: 6.683592319488525\n",
            "Epoch: 3, Batch: 247, D Loss: 0.18067973852157593, G Loss: 6.686469554901123\n",
            "Epoch: 3, Batch: 248, D Loss: 0.18061217665672302, G Loss: 6.689281940460205\n",
            "Epoch: 3, Batch: 249, D Loss: 0.18054160475730896, G Loss: 6.692002296447754\n",
            "Epoch: 3, Batch: 250, D Loss: 0.18047446012496948, G Loss: 6.694752216339111\n",
            "Epoch: 3, Batch: 251, D Loss: 0.1804027557373047, G Loss: 6.697498798370361\n",
            "Epoch: 3, Batch: 252, D Loss: 0.18033427000045776, G Loss: 6.700263023376465\n",
            "Epoch: 3, Batch: 253, D Loss: 0.1802653670310974, G Loss: 6.703039169311523\n",
            "Epoch: 3, Batch: 254, D Loss: 0.18020093441009521, G Loss: 6.705874919891357\n",
            "Epoch: 3, Batch: 255, D Loss: 0.18013569712638855, G Loss: 6.708733558654785\n",
            "Epoch: 3, Batch: 256, D Loss: 0.18006718158721924, G Loss: 6.711487770080566\n",
            "Epoch: 3, Batch: 257, D Loss: 0.18000465631484985, G Loss: 6.714260578155518\n",
            "Epoch: 3, Batch: 258, D Loss: 0.17993828654289246, G Loss: 6.717031002044678\n",
            "Epoch: 3, Batch: 259, D Loss: 0.17987433075904846, G Loss: 6.719809532165527\n",
            "Epoch: 3, Batch: 260, D Loss: 0.17980487644672394, G Loss: 6.7224812507629395\n",
            "Epoch: 3, Batch: 261, D Loss: 0.17973938584327698, G Loss: 6.725148677825928\n",
            "Epoch: 3, Batch: 262, D Loss: 0.17967283725738525, G Loss: 6.727903842926025\n",
            "Epoch: 3, Batch: 263, D Loss: 0.17960196733474731, G Loss: 6.730594158172607\n",
            "Epoch: 3, Batch: 264, D Loss: 0.17953169345855713, G Loss: 6.7332024574279785\n",
            "Epoch: 3, Batch: 265, D Loss: 0.17946407198905945, G Loss: 6.735830783843994\n",
            "Epoch: 3, Batch: 266, D Loss: 0.17939594388008118, G Loss: 6.738514423370361\n",
            "Epoch: 3, Batch: 267, D Loss: 0.17933142185211182, G Loss: 6.7412824630737305\n",
            "Epoch: 3, Batch: 268, D Loss: 0.17925997078418732, G Loss: 6.743947505950928\n",
            "Epoch: 3, Batch: 269, D Loss: 0.17919902503490448, G Loss: 6.746677398681641\n",
            "Epoch: 3, Batch: 270, D Loss: 0.17912760376930237, G Loss: 6.749323844909668\n",
            "Epoch: 3, Batch: 271, D Loss: 0.17906275391578674, G Loss: 6.751949310302734\n",
            "Epoch: 3, Batch: 272, D Loss: 0.17900046706199646, G Loss: 6.754664421081543\n",
            "Epoch: 3, Batch: 273, D Loss: 0.17893417179584503, G Loss: 6.757390022277832\n",
            "Epoch: 3, Batch: 274, D Loss: 0.17886650562286377, G Loss: 6.760016441345215\n",
            "Epoch: 3, Batch: 275, D Loss: 0.17880040407180786, G Loss: 6.762605667114258\n",
            "Epoch: 3, Batch: 276, D Loss: 0.1787373572587967, G Loss: 6.765295028686523\n",
            "Epoch: 3, Batch: 277, D Loss: 0.17867411673069, G Loss: 6.768040657043457\n",
            "Epoch: 3, Batch: 278, D Loss: 0.17861241102218628, G Loss: 6.770802974700928\n",
            "Epoch: 3, Batch: 279, D Loss: 0.17854362726211548, G Loss: 6.77339506149292\n",
            "Epoch: 3, Batch: 280, D Loss: 0.17847943305969238, G Loss: 6.775964260101318\n",
            "Epoch: 3, Batch: 281, D Loss: 0.1784207969903946, G Loss: 6.778759479522705\n",
            "Epoch: 3, Batch: 282, D Loss: 0.17834970355033875, G Loss: 6.781536102294922\n",
            "Epoch: 3, Batch: 283, D Loss: 0.17829063534736633, G Loss: 6.784304618835449\n",
            "Epoch: 3, Batch: 284, D Loss: 0.17822417616844177, G Loss: 6.786933422088623\n",
            "Epoch: 3, Batch: 285, D Loss: 0.17815807461738586, G Loss: 6.789520263671875\n",
            "Epoch: 3, Batch: 286, D Loss: 0.17809003591537476, G Loss: 6.792183876037598\n",
            "Epoch: 3, Batch: 287, D Loss: 0.17802564799785614, G Loss: 6.794947624206543\n",
            "Epoch: 3, Batch: 288, D Loss: 0.1779584288597107, G Loss: 6.7976975440979\n",
            "Epoch: 3, Batch: 289, D Loss: 0.17789480090141296, G Loss: 6.8003740310668945\n",
            "Epoch: 3, Batch: 290, D Loss: 0.17782878875732422, G Loss: 6.802997589111328\n",
            "Epoch: 3, Batch: 291, D Loss: 0.17776359617710114, G Loss: 6.805615425109863\n",
            "Epoch: 3, Batch: 292, D Loss: 0.17770040035247803, G Loss: 6.808305263519287\n",
            "Epoch: 3, Batch: 293, D Loss: 0.1776379495859146, G Loss: 6.811028003692627\n",
            "Epoch: 3, Batch: 294, D Loss: 0.17757399380207062, G Loss: 6.813708305358887\n",
            "Epoch: 3, Batch: 295, D Loss: 0.1775083839893341, G Loss: 6.816302299499512\n",
            "Epoch: 3, Batch: 296, D Loss: 0.1774461567401886, G Loss: 6.818934440612793\n",
            "Epoch: 3, Batch: 297, D Loss: 0.17738060653209686, G Loss: 6.821617603302002\n",
            "Epoch: 3, Batch: 298, D Loss: 0.17731507122516632, G Loss: 6.824289798736572\n",
            "Epoch: 3, Batch: 299, D Loss: 0.1772555708885193, G Loss: 6.827029228210449\n",
            "Epoch: 3, Batch: 300, D Loss: 0.17719125747680664, G Loss: 6.829756736755371\n",
            "Epoch: 3, Batch: 301, D Loss: 0.17712810635566711, G Loss: 6.832442283630371\n",
            "Epoch: 3, Batch: 302, D Loss: 0.17706632614135742, G Loss: 6.835136890411377\n",
            "Epoch: 3, Batch: 303, D Loss: 0.17700058221817017, G Loss: 6.837825298309326\n",
            "Epoch: 3, Batch: 304, D Loss: 0.17694050073623657, G Loss: 6.84058952331543\n",
            "Epoch: 3, Batch: 305, D Loss: 0.17687931656837463, G Loss: 6.843398571014404\n",
            "Epoch: 3, Batch: 306, D Loss: 0.17681682109832764, G Loss: 6.846156597137451\n",
            "Epoch: 3, Batch: 307, D Loss: 0.17675668001174927, G Loss: 6.848879337310791\n",
            "Epoch: 3, Batch: 308, D Loss: 0.17669080197811127, G Loss: 6.851529598236084\n",
            "Epoch: 3, Batch: 309, D Loss: 0.1766248345375061, G Loss: 6.85415506362915\n",
            "Epoch: 3, Batch: 310, D Loss: 0.1765638291835785, G Loss: 6.856904029846191\n",
            "Epoch: 3, Batch: 311, D Loss: 0.1765025109052658, G Loss: 6.859744548797607\n",
            "Epoch: 3, Batch: 312, D Loss: 0.17644064128398895, G Loss: 6.86253547668457\n",
            "Epoch: 3, Batch: 313, D Loss: 0.17637747526168823, G Loss: 6.865164279937744\n",
            "Epoch: 3, Batch: 314, D Loss: 0.17631521821022034, G Loss: 6.867745876312256\n",
            "Epoch: 3, Batch: 315, D Loss: 0.17625506222248077, G Loss: 6.870460510253906\n",
            "Epoch: 3, Batch: 316, D Loss: 0.17619170248508453, G Loss: 6.873201847076416\n",
            "Epoch: 3, Batch: 317, D Loss: 0.17612913250923157, G Loss: 6.87581205368042\n",
            "Epoch: 3, Batch: 318, D Loss: 0.17607104778289795, G Loss: 6.878422260284424\n",
            "Epoch: 3, Batch: 319, D Loss: 0.17600537836551666, G Loss: 6.881000995635986\n",
            "Epoch: 3, Batch: 320, D Loss: 0.1759498417377472, G Loss: 6.883732318878174\n",
            "Epoch: 3, Batch: 321, D Loss: 0.1758892834186554, G Loss: 6.88646125793457\n",
            "Epoch: 3, Batch: 322, D Loss: 0.17583246529102325, G Loss: 6.889125823974609\n",
            "Epoch: 3, Batch: 323, D Loss: 0.17577046155929565, G Loss: 6.891642093658447\n",
            "Epoch: 3, Batch: 324, D Loss: 0.17571011185646057, G Loss: 6.89414644241333\n",
            "Epoch: 3, Batch: 325, D Loss: 0.17564405500888824, G Loss: 6.896663665771484\n",
            "Epoch: 3, Batch: 326, D Loss: 0.17558521032333374, G Loss: 6.899337291717529\n",
            "Epoch: 3, Batch: 327, D Loss: 0.17552459239959717, G Loss: 6.9020233154296875\n",
            "Epoch: 3, Batch: 328, D Loss: 0.17546525597572327, G Loss: 6.904613971710205\n",
            "Epoch: 3, Batch: 329, D Loss: 0.17540308833122253, G Loss: 6.907068252563477\n",
            "Epoch: 3, Batch: 330, D Loss: 0.17534536123275757, G Loss: 6.909578800201416\n",
            "Epoch: 3, Batch: 331, D Loss: 0.17528264224529266, G Loss: 6.912172317504883\n",
            "Epoch: 3, Batch: 332, D Loss: 0.17522403597831726, G Loss: 6.914843559265137\n",
            "Epoch: 3, Batch: 333, D Loss: 0.1751689612865448, G Loss: 6.917538166046143\n",
            "Epoch: 3, Batch: 334, D Loss: 0.17510828375816345, G Loss: 6.920086860656738\n",
            "Epoch: 3, Batch: 335, D Loss: 0.17504657804965973, G Loss: 6.92252779006958\n",
            "Epoch: 3, Batch: 336, D Loss: 0.17498072981834412, G Loss: 6.924914360046387\n",
            "Epoch: 3, Batch: 337, D Loss: 0.17492102086544037, G Loss: 6.927484512329102\n",
            "Epoch: 3, Batch: 338, D Loss: 0.17486156523227692, G Loss: 6.930190563201904\n",
            "Epoch: 3, Batch: 339, D Loss: 0.174800843000412, G Loss: 6.932828426361084\n",
            "Epoch: 3, Batch: 340, D Loss: 0.1747462898492813, G Loss: 6.935453414916992\n",
            "Epoch: 3, Batch: 341, D Loss: 0.17469079792499542, G Loss: 6.9380974769592285\n",
            "Epoch: 3, Batch: 342, D Loss: 0.17463144659996033, G Loss: 6.94070291519165\n",
            "Epoch: 3, Batch: 343, D Loss: 0.17456895112991333, G Loss: 6.9431891441345215\n",
            "Epoch: 3, Batch: 344, D Loss: 0.17451484501361847, G Loss: 6.945801258087158\n",
            "Epoch: 3, Batch: 345, D Loss: 0.17445313930511475, G Loss: 6.948432445526123\n",
            "Epoch: 3, Batch: 346, D Loss: 0.17439529299736023, G Loss: 6.951049327850342\n",
            "Epoch: 3, Batch: 347, D Loss: 0.17433451116085052, G Loss: 6.953566074371338\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1385464696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Call the train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1066119254.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mz_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mX_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_zeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Discriminator loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    226\u001b[0m                     \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 ):\n\u001b[0;32m--> 228\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**\n",
        "- Epochs parameter determines how many times the learning algorithm will work through the entire training dataset.\n",
        "- The `batch_size` is the number of samples that will be propagated through the network at a time."
      ],
      "metadata": {
        "id": "I09QJRDIlZqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 7: Generate New Images and Evaluate the Model's Performance__\n",
        "\n",
        "- Generate new images and evaluate the performance of the GAN.\n",
        "- Generate a random noise vector and feed it into the trained generator to create new images."
      ],
      "metadata": {
        "id": "ZtmQnlKeg9C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Generate random noise as an input to initialize the generator\n",
        "random_noise = np.random.normal(0,1, [100, 100])\n",
        "\n",
        "# Generate the images from the noise\n",
        "generated_images = generator.predict(random_noise)\n",
        "\n",
        "# Visualize the generated images\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(generated_images.shape[0]):\n",
        "    plt.subplot(10, 10, i+1)\n",
        "    plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oIlF3BEEe45G",
        "outputId": "7d69ba3f-0d68-4aa0-dfed-75515819180f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 9ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 100 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Conclusion__\n",
        "\n",
        "In this demo, you have successfully implemented a GAN to generate images resembling handwritten digits, focusing on the MNIST dataset. The process involved constructing and training a generator and a discriminator. The results were promising, showcasing the GAN's ability to create images similar to the digit **8**, but also highlighted the need for further improvements in image quality."
      ],
      "metadata": {
        "id": "Ic6p2uAFJMDl"
      }
    }
  ]
}